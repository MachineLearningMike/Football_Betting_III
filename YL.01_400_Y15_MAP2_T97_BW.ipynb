{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Checklist\n",
    "1. Make sure your \"Regional Format\" is set to \"English (United Kingdom)\" before Excel opens files that contain English date-like objects,\n",
    "   if you want them to be parsed as English datetime. English (United States) regional format will convert them to American Datetime as possible.\n",
    "   You have no way to prevent Excel from converting date-like strings or objects to datetime when opening a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "from config import config\n",
    "import YK_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "\n",
    "OPERATION = 'PRETRAIN'  # 'PRETRAIN'  'TRAIN_C'   'FINETUNE'   'TEST', 'BACKTEST'\n",
    "\n",
    "MODEL_ID = 'YK.02'\n",
    "HISTORY_LEN = 400\n",
    "SEQ_YEAR_SPAN = 15\n",
    "NOTE = 'TempTEST'\n",
    "\n",
    "DATA_THEME = str(HISTORY_LEN).zfill(3) + '-' + str(SEQ_YEAR_SPAN).zfill(2) + '-' + NOTE\n",
    "COPY_IDS_DS_DATA_FROM = None  # \"_DATA_400-15-Test\"  None\n",
    "\n",
    "NUMBER_QUERIES = 3\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 0\n",
    "TRAIN_PERCENT= 97   #------ 96\n",
    "VALID_PERCENT = 3   #------ 4\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT\n",
    "FILTER_FOR_GOOD_DATA = True\n",
    "\n",
    "BATCH_SIZE = 25 # 25 for headers 6.\n",
    "\n",
    "TEAM_EMBS = 50  #\n",
    "DECODE_BASE_DATE = False\n",
    "EMBED_AB_COLS = False    # True: Pls choose a small LR so that we have plenty of train epochs and the embedded values have enough chance to seek their proper places.\n",
    "ODDS_IN_ENCODER = False   #--------------\n",
    "ODDS_IN_DECODER = True\n",
    "ADDITIONAL_D_MODEL = 0   #------------ Try increase it when underfitting.\n",
    "\n",
    "TRANSFORMER_LAYERS = 30        #----------30\n",
    "TRANSFORMER_HEADS = 6\n",
    "BALANCE_POS_CODE = True        # True: Weakens positional code compared to embedded values.\n",
    "DROPOUT = 0.0  # 0.1\n",
    "ADAPTORS_LAYERS = 0 #------------ 10\n",
    "ADAPTORS_WIDTH_FACTOR = 30  # 30\n",
    "\n",
    "# This is an exponential curve that hits STARTING_LEARNING_RATE at step zero and EXAMPLE_LEARNING_RATE at step EXAMPLE_LEARNING_STEP.\n",
    "# lr(step) = STARTING_LEARNING_RATE * pow( pow(EXAMPLE_LEARNING_RATE/STARTING_LEARNING_RATE, 1/EXAMPLE_LEARNING_STEP), step )\n",
    "STARTING_LEARNING_RATE = 5e-7\n",
    "EXAMPLE_LEARNING_RATE = STARTING_LEARNING_RATE * 0.5    #------------ 0.5 worked.\n",
    "EXAMPLE_LEARNING_STEP = 100\n",
    "\n",
    "MODEL_ACTIVATION = 'softmax'    # 'softmax', 'sigmoid', 'relu', 'open'\n",
    "LOSS_TYPE = 'profit'           # 'profit', 'entropy'\n",
    "VECTOR_BETTING = False\n",
    "STAKE_TECHNIQUE = 'independent'         # 'independent', 'majority', 'riskest'\n",
    "\n",
    "MODEL_TYPE_CHECK = False\n",
    "GET_VAL_LOSS_0 = False\n",
    "IGNORE_HISTORY = False\n",
    "SIMPLIFY_ADAPTOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = 'England'\n",
    "FOCUS_BOOKIE = 1    # None\n",
    "NUMBER_BOOKIES = 5  # Take William Hills, Bet&Win, and Bet365. Other bookies' odds list change over years and leagues.\n",
    "PREFERED_ORDER = ['B365', 'WH']   # Implies the order only.\n",
    "BOOKIE_TO_EXCLUDE = []    # 'BWIN' odds don't show up since mid Febrary 2025. This may reduce the effective NUMBER_BOOKIES.\n",
    "DIVIISONS = ['E0', 'E1', 'E2', 'E3']    # 'EC', the Conference league, is excluded as some odds makers are not archived for the league since 2013.\n",
    "REPEATING_BOOKIE = None    #-----------------------------------\n",
    "\n",
    "\"\"\"\n",
    "#-------------------- England ---------------------\n",
    "Time range of data: 2004/2005 - 2024/2025\n",
    "Leagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\n",
    "!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\n",
    "Bookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\n",
    "William Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\n",
    "William Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\n",
    "William Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\n",
    "William Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\n",
    "William Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\n",
    "BWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\n",
    "\"\"\"\n",
    "\n",
    "# COUNTRY = 'Scotland'\n",
    "# NUMBER_BOOKIES = 3  # Take ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed): np.random.seed(seed); tf.random.set_seed(seed); random.seed(seed)\n",
    "set_seed(23)    # For serendipity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_COLS = []\n",
    "for b in range(NUMBER_BOOKIES):\n",
    "    ODDS_COLS += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ODDS_COLS\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "Required_Non_Odds_cols = Div_cols + Date_cols + Team_cols + Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "\n",
    "# Make sure Odds_cols comes first !!!\n",
    "_Cols_to_Always_Normalize = Odds_cols\n",
    "\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n",
    "\n",
    "bbab_odds_start = BBAB_cols.index(Odds_cols[0])\n",
    "bb_odds_start = BB_cols.index(Odds_cols[0])\n",
    "label_odds_start = _Label_cols.index(Odds_cols[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BBAB_cols)\n",
    "print(BB_cols)\n",
    "print(_Label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_folder = os.path.join('.', 'history')\n",
    "country_folder = os.path.join('.', \"data\", \"football-data-co-uk\", COUNTRY)\n",
    "gameHistory_folder_path = os.path.join(country_folder, 'Game_History')\n",
    "if not os.path.exists(gameHistory_folder_path): os.makedirs(gameHistory_folder_path, exist_ok=False)\n",
    "countryTheme_folder_path = os.path.join(country_folder, '_DATA_' + DATA_THEME)\n",
    "if not os.path.exists(countryTheme_folder_path): os.makedirs(countryTheme_folder_path, exist_ok=False)\n",
    "map_folder_path = os.path.join(countryTheme_folder_path, '_id_map')     # do not change to '_map' for backward compatibility\n",
    "if not os.path.exists(map_folder_path): os.makedirs(map_folder_path, exist_ok=False)\n",
    "dataset_foler_path = os.path.join(countryTheme_folder_path, '_dataaset')    # keep '_dataaset' as it for backward compatibility\n",
    "if not os.path.exists(dataset_foler_path): os.makedirs(dataset_foler_path, exist_ok=False)\n",
    "checkpoint_folder_path = os.path.join(country_folder, '_checkpoints')\n",
    "if not os.path.exists(checkpoint_folder_path): os.makedirs(checkpoint_folder_path, exist_ok=False)\n",
    "tokenizer_folder_path =  os.path.join(countryTheme_folder_path, '_tokenizer')\n",
    "if not os.path.exists(tokenizer_folder_path): os.makedirs(tokenizer_folder_path, exist_ok=False)\n",
    "map_filename_base = DATA_THEME\n",
    "dataset_path = os.path.join(dataset_foler_path, map_filename_base + '-foundation-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================= CREATE_DADAFRAMES_v2 =============================\n",
    "\n",
    "skip = False\n",
    "YK_assistant.assign_seasonal_filenames(gameHistory_folder_path)\n",
    "df_train, df_new = YK_assistant.CREATE_DADAFRAMES_v2(gameHistory_folder_path, countryTheme_folder_path, Required_Non_Odds_cols, NUMBER_BOOKIES, oddsGroupsToExclude = BOOKIE_TO_EXCLUDE, preferedOrder = PREFERED_ORDER, train_mode = TRAIN_MODE, skip=skip)\n",
    "print(\"df_train: \", df_train.shape)\n",
    "print(\"df_new: \", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_team = YK_assistant.creat_team_tokenizer_uk(df_train, tokenizer_folder_path)\n",
    "team_vocab = list(tokenizer_team.get_vocab().keys())\n",
    "\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "print(tokenizer_team.get_vocab())\n",
    "print(team_vocab)\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "print(tokenizer_team.decode(encoding.ids))\n",
    "teams, success = YK_assistant.convert_to_token(tokenizer_team, ['Manchester United', 'Man united', 'man_united', 'chelsea'])\n",
    "print(teams)\n",
    "\n",
    "similars = YK_assistant.find_most_similar_teams(team_vocab, 'manchester united', 3)\n",
    "print(similars)\n",
    "\n",
    "teams, candidates, success = YK_assistant.convert_to_token_with_candidates(tokenizer_team, ['mman United', 'Liberpool', 'chelsea'], candi_count=3)\n",
    "print(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE and COPY_IDS_DS_DATA_FROM is not None:\n",
    "    YK_assistant.remove_folder_contents(map_folder_path)\n",
    "    source_map_folder_path = os.path.join(country_folder, COPY_IDS_DS_DATA_FROM, '_id_map')\n",
    "    YK_assistant.copy_id_map(source_map_folder_path, map_folder_path, NOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= CREATE_MAP_v2 ========================\n",
    "\n",
    "if TRAIN_MODE:\n",
    "    targetLength = HISTORY_LEN\n",
    "\n",
    "    df_sequence = df_train; df_base = df_train\n",
    "    train_map = YK_assistant.CREATE_MAP_v2(map_folder_path, map_filename_base, targetLength, df_sequence, df_base, year_span=SEQ_YEAR_SPAN, testcount=-1)\n",
    "\n",
    "    print(\"\\nlen = \", len(train_map))\n",
    "\n",
    "    seq_types = [sType for (sType, _) in train_map.values()]\n",
    "    seq_types = list(set([(seq_types.count(sType), sType) for sType in seq_types]))\n",
    "    seq_types.sort()\n",
    "    print(seq_types)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [len(ids) for (report, ids) in train_map.values()]\n",
    "# maxLen = max(lengths)\n",
    "# plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "# plt.ylim(plt.ylim())\n",
    "# maxLen = max(lengths)\n",
    "# # plt.plot([maxLen, maxLen], plt.ylim())\n",
    "# plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "# lengths = [len(ids) for (report, ids) in train_map.values()]\n",
    "# maxLen = max(lengths)\n",
    "# plt.hist(lengths, np.linspace(0, int(maxLen*0.9), int(maxLen*0.9) + 1))\n",
    "# plt.ylim(plt.ylim())\n",
    "# maxLen = max(lengths)\n",
    "# # plt.plot([maxLen, maxLen], plt.ylim())\n",
    "# plt.title(f'Max length of ids: {maxLen}')\n",
    "# assert maxLen == HISTORY_LEN\n",
    "\n",
    "MAX_TOKENS = HISTORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_raw_bbab(bbab, tokenizer_team, normalization_parms):\n",
    "\n",
    "    label = []\n",
    "    #---------------------- label, before changing bbab. They are raw full-time goals and raw odds.\n",
    "    for col in _Label_cols:\n",
    "        start = BBAB_cols.index(col)\n",
    "        label.append(bbab[start])\n",
    "\n",
    "    #----------------------- \n",
    "    start = BBAB_cols.index(Div_cols[0])\n",
    "    Div = bbab[start]\n",
    "    bbab[start] = DIVIISONS.index(Div)  # Assumes no n/a\n",
    "\n",
    "    start = BBAB_cols.index(Team_cols[0]); end = BBAB_cols.index(Team_cols[-1]) + 1\n",
    "    pair_str = [str(team) for team in bbab[start : end]]    # Team names are already normalized, removing/striping spaces.\n",
    "    pair_text = \" \".join(pair_str)\n",
    "    pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "    bbab[start : end] = pair_tokens # 0 for Unknown, by tokenizer trainig.\n",
    "\n",
    "    #--------------------- normalize\n",
    "    for col in _Cols_to_Always_Normalize:   # Odds_cols only now.\n",
    "        (mean, std, maximum) = normalization_parms[col]\n",
    "        start = BBAB_cols.index(col)\n",
    "        bbab[start] = (bbab[start] - mean) / std\n",
    "\n",
    "    #--------------------- columns for positional embedding\n",
    "    start = BBAB_cols.index(Date_cols[0])   #\n",
    "    date = bbab[start]\n",
    "    bbab[start] = (datetime.datetime.combine(date, datetime.time(0,0,0)) - config['baseDate']).days  # either positive or negative\n",
    "\n",
    "    #---------------------- bb only\n",
    "    start = BBAB_cols.index(BB_cols[0]); end = start + len(BB_cols)     # \n",
    "    bb = bbab[start : end]\n",
    "\n",
    "    return bbab, bb, label, date\n",
    "\n",
    "def getDateDetails(date):\n",
    "    baseYear = config['baseDate'].year\n",
    "    date_details = tf.Variable([date.year - baseYear, date.month, date.day, date.weekday()], dtype=tf.int32, trainable=False)\n",
    "    return date_details     # (4,)\n",
    "\n",
    "filler = tf.zeros_like([0] * len(BBAB_cols), dtype=tf.float32)\n",
    "\n",
    "def get_data_record(df_sequence, df_base, baseId, ids, tokenizer_team, normalization_parms):\n",
    "    # try:\n",
    "        # base_bbab = list(df_train.loc[df_train['id'] == baseId, BBAB_cols])\n",
    "        base_bbab = list(df_base[df_base['id'] == baseId][BBAB_cols].iloc[0, :])  # base_bbab follows BBAB. list------\n",
    "        base_bbab, base_bb, base_label, base_date = standardize_raw_bbab(base_bbab, tokenizer_team, normalization_parms)\n",
    "        # base_bbab, base_bb, base_label, base_date\n",
    "        baseId = tf.Variable(baseId, dtype=tf.int32, trainable=False)\n",
    "        base_bbab = tf.Variable(base_bbab, dtype=tf.float32, trainable=False)    # (len(BBAB_cols),)\n",
    "        base_bb = tf.Variable(base_bb, dtype=tf.float32, trainable=False)        # (len(BB_cols),)\n",
    "        base_label = tf.Variable(base_label, dtype=tf.float32, trainable=False)  # (len(_Label_cols),)\n",
    "        # print('3', base_bbab)\n",
    "        # Default sequence.\n",
    "        sequence = tf.transpose(tf.Variable([[]] * len(BBAB_cols), dtype=tf.float32, trainable=False))   # (0, len(BBAB_cols))\n",
    "        # sequence = np.array([[]] * len(BBAB_cols), dtype=config['np_float']).T\n",
    "        # print('3.5', sequence)\n",
    "        baseDateDetails = getDateDetails(base_date) # (4,)\n",
    "\n",
    "        concat = []\n",
    "        for id in ids:\n",
    "            bbab = list(df_sequence[df_sequence['id'] == id][BBAB_cols].iloc[0, :])   # bbab follows BBAB. list\n",
    "            bbab, _, _, _ = standardize_raw_bbab(bbab, tokenizer_team, normalization_parms)   # bbab follows BBAB. list\n",
    "            # check_normalization(bbab, normalization_parms)\n",
    "\n",
    "            bbab = tf.Variable(bbab, dtype=tf.float32, trainable=False)[tf.newaxis, :]       # (1, len(BBAB_cols))\n",
    "            # _bbab = bbab[0].numpy()\n",
    "            # check_normalization(_bbab, normalization_parms)\n",
    "\n",
    "            concat.append(bbab)     # concat doesn't create a new axis.\n",
    "\n",
    "        if len(concat) > 0:\n",
    "            sequence = tf.concat(concat, axis=0)    # (nSequence, len(BBAB_cols))\n",
    "            # if sequence.shape[0] > 0:\n",
    "            #     bbab = sequence[0].numpy()\n",
    "            #     check_normalization(bbab, normalization_parms)\n",
    "\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            patch = tf.stack([filler] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, patch], axis=0)     # concat doesn't create a new axis. (MAX_TOKENS, len(BBAB_cols))\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, len(BBAB_cols))\n",
    "        baseDateDetails = baseDateDetails[tf.newaxis, :]\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32, trainable=False) # (MAX_TOKENS,) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]    # (MAX_TOKENS, MAX_TOKENS)\n",
    "\n",
    "        return (baseId, sequence, base_bb, base_label, baseDateDetails, mask)\n",
    "\n",
    "\n",
    "def CREATE_DATASET(df_sequence, df_base, existing_ds, total_map, tokenizer_team, normalization_parms):\n",
    "    def generator():\n",
    "        total_baseIds = [int(id) for id in list(total_map.keys())]\n",
    "\n",
    "        print('total_baseIds', total_baseIds)\n",
    "\n",
    "        max_id = -1; count = 0\n",
    "        if existing_ds is not None : # and len(existing_ds) > 0:\n",
    "            for z in existing_ds:\n",
    "                (baseId, _, _, _, _, _, _) = z\n",
    "                baseId = baseId.numpy()\n",
    "                if baseId in total_baseIds:\n",
    "                    if max_id < baseId: max_id = baseId\n",
    "                    print(\"count: {}, existing baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "                    yield z\n",
    "        new_map = { id : value for (id, value) in total_map.items() if max_id < int(id)}\n",
    "\n",
    "        for baseId, (seq_type, ids) in new_map.items():\n",
    "            baseId = int(baseId)\n",
    "            baseId, sequence, base_bb, base_label, baseDateDetails, mask = get_data_record(df_sequence, df_base, baseId, ids, tokenizer_team, normalization_parms)\n",
    "            print(\"count: {}, new baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "            # if count > 200: break\n",
    "            yield (baseId, seq_type, sequence, base_bb, base_label, baseDateDetails, mask)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS))),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def repeat_odds(sequence, base_bb, base_label, repeating_bookie):\n",
    "    # sequence: (batch, max_seq_len, bbab_cols)\n",
    "    # base_bb:  (batch, 1, bb_cols)\n",
    "    # base_label: (batch, _Label_cols)\n",
    "    def repeat_odds3(tensor, oddsStart):\n",
    "        seedStart = oddsStart + repeating_bookie * NUMBER_QUERIES\n",
    "        seedEnd = seedStart + NUMBER_QUERIES\n",
    "        oddsSeed = tensor[:, :, seedStart : seedEnd]\n",
    "        odds = tf.concat([oddsSeed] * NUMBER_BOOKIES, axis=-1)\n",
    "        tensor = tf.concat( [tensor[:, :, : oddsStart], odds, tensor[:, :, oddsStart + NUMBER_BOOKIES * NUMBER_QUERIES :] ], axis=-1)\n",
    "        return tensor\n",
    "    \n",
    "    def repeat_odds2(tensor, oddsStart):\n",
    "        seedStart = oddsStart + repeating_bookie * NUMBER_QUERIES\n",
    "        seedEnd = seedStart + NUMBER_QUERIES\n",
    "        oddsSeed = tensor[:, seedStart : seedEnd]\n",
    "        odds = tf.concat([oddsSeed] * NUMBER_BOOKIES, axis=-1)\n",
    "        tensor = tf.concat( [tensor[:, : oddsStart], odds, tensor[:, oddsStart + NUMBER_BOOKIES * NUMBER_QUERIES :] ], axis=-1)\n",
    "        return tensor\n",
    "\n",
    "    sequence = repeat_odds3(sequence, bbab_odds_start)\n",
    "    base_bb = repeat_odds3(base_bb, bb_odds_start)\n",
    "    base_label = repeat_odds2(base_label, label_odds_start)\n",
    "\n",
    "    return sequence, base_bb, base_label\n",
    "\n",
    "   \n",
    "def X_versus_Y(baseId, seq_type, sequence, base_bb, base_label, baseDateDetails, mask):\n",
    "    if REPEATING_BOOKIE is not None:\n",
    "        sequence, base_bb, base_label = repeat_odds(sequence, base_bb, base_label, REPEATING_BOOKIE)\n",
    "    return (baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_type)\n",
    "\n",
    "# I found, in PositionalEmbedding, batch size ranges between 3, 4, 6 and 8, while they should be 4 or 8, except margial rows. Check it.\n",
    "train_batch_size = BATCH_SIZE\n",
    "test_batch_size = BATCH_SIZE * 2\n",
    "\n",
    "def apply_train_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(ds.cardinality(), reshuffle_each_iteration=True)       # reshuffle doesn't happen at python iteration(ds)\n",
    "        .batch(train_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def apply_test_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(test_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COPY_IDS_DS_DATA_FROM is not None:\n",
    "    YK_assistant.remove_folder_contents(dataset_foler_path)\n",
    "    source_map_folder_path = os.path.join(country_folder, COPY_IDS_DS_DATA_FROM, '_dataaset')\n",
    "    YK_assistant.copy_dataset(source_map_folder_path, dataset_foler_path, NOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_path = os.path.join(dataset_foler_path, map_filename_base + '-normalization' + \".json\")\n",
    "normalization_parms = YK_assistant.get_normalization_params(df_train, cols = _Cols_to_Always_Normalize + AB_cols)\n",
    "print(normalization_parms)\n",
    "YK_assistant.SaveJsonData(normalization_parms, std_path)\n",
    "normalization_parms = YK_assistant.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================= CREATE_DATASET ===========================\n",
    "\n",
    "skip = True\n",
    "if TRAIN_MODE:\n",
    "    if os.path.exists(dataset_path):\n",
    "        existing_ds = tf.data.Dataset.load(dataset_path, compression='GZIP')\n",
    "        if skip == True: total_ds = existing_ds\n",
    "        else:\n",
    "            for z in existing_ds: break      # Required. \"for z in ds\" in generator will behavor as if ds is empty, if we remove this line.\n",
    "            print(\"with existing ds...\", len(existing_ds))\n",
    "            df_sequence = df_train; df_base = df_train; total_map = train_map\n",
    "            total_ds = CREATE_DATASET(df_sequence, df_base, existing_ds, total_map, tokenizer_team, normalization_parms)    # defines, not builds, total_ds.\n",
    "            existing_data_subfolders = YK_assistant.get_subfolders(dataset_path)\n",
    "            tf.data.Dataset.save(total_ds, dataset_path, compression='GZIP')       # builds what total_ds defines.\n",
    "            total_ds = tf.data.Dataset.load(dataset_path) #, compression='GZIP')   # assign dataset to total_ds.\n",
    "            YK_assistant.remove_subfolders(dataset_path, existing_data_subfolders)\n",
    "    else:\n",
    "        print('without existing ds...')\n",
    "        df_sequence = df_train; df_base = df_train; existing_ds = None; total_map = train_map\n",
    "        total_ds = CREATE_DATASET(df_sequence, df_base, existing_ds, total_map, tokenizer_team, normalization_parms)    # defines, not builds, total_ds.\n",
    "        tf.data.Dataset.save(total_ds, dataset_path, compression='GZIP')       # builds what total_ds defines.\n",
    "        total_ds = tf.data.Dataset.load(dataset_path, compression='GZIP')      # assign dataset to total_ds.\n",
    "\n",
    "    print(\"The length of total dataset: {}       \".format(len(total_ds)))\n",
    "\n",
    "    # len(total_ds) doesn't work, after applying a filter.\n",
    "    # if FILTER_FOR_GOOD_DATA: \n",
    "    #     total_ds = total_ds.filter(lambda baseId, seq_type, sequence, base_bb, base_label, baseDateDetails, mask: seq_type == 20)\n",
    "    #     for z in total_ds: pass\n",
    "    #     print(\"The length of total dataset after filtering: {}       \".format(len(total_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE:\n",
    "      total_size = len(total_ds)\n",
    "\n",
    "      # ds = ds.shuffle(ds.cardinality(), seed=523) # Assuming the data is time-independent. seed does NOT work.\n",
    "\n",
    "      starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "      starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "      ending_size = int(ENDING_PERCENT/100 * total_size)\n",
    "      take_size = total_size - starting_size - ending_size\n",
    "      remaining_ds = total_ds.skip(starting_size)\n",
    "      dataset = remaining_ds.take(take_size)          # starting | dataset | ending\n",
    "\n",
    "      dataset_size = len(dataset)\n",
    "      train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "      valid_size = int(VALID_PERCENT/100 * dataset_size)\n",
    "      test_size = dataset_size - train_size - valid_size      # len(dataset) = train_size + valid_size + tast_size        NO back_size\n",
    "      train_ds = dataset.take(train_size)                    # dataset[: train_size]\n",
    "      remaining_ds = dataset.skip(train_size - valid_size)    # dataset[train_size - valid_size: ]\n",
    "      back_ds = remaining_ds.take(valid_size)                # dataset[train_size - valid_size: train_size]\n",
    "      remaining_ds = remaining_ds.skip(valid_size)            # dataset[train_size: ]\n",
    "      valid_ds = remaining_ds.take(valid_size)               # dataset[train_size, train_size + valid_size]\n",
    "      remaining_ds = remaining_ds.skip(valid_size)                # dataset[train_size + valid_size :]\n",
    "      test_ds = remaining_ds.take(test_size)\n",
    "      backtest_ds = valid_ds.concatenate(test_ds) # tf.data.Dataset.zip((valid_ds, test_ds))\n",
    "\n",
    "      assert len(test_ds) == test_size\n",
    "      assert dataset_size == len(train_ds) + len(valid_ds) + len(test_ds)\n",
    "\n",
    "      print(\"total_size, dataset, train_ds, back_ds, valid_ds, test_ds, backtest_ds: \", \\\n",
    "            total_size, len(dataset), len(train_ds), len(back_ds), len(valid_ds), len(test_ds), len(backtest_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['baseDate'] = datetime.datetime(2000, 1, 1)\n",
    "dt = config['baseDate']\n",
    "dt.year\n",
    "dt = datetime.datetime(config['baseDate'].year + 5, 3, 15)\n",
    "dt\n",
    "dt2 = datetime.datetime(config['baseDate'].year + 10, 3, 15)\n",
    "td = dt2 - dt\n",
    "td.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN_MODE:\n",
    "    train_batches = apply_train_pipeline(train_ds)\n",
    "    back_batches = apply_test_pipeline(back_ds)\n",
    "    valid_batches = apply_test_pipeline(valid_ds)\n",
    "    test_batches = apply_test_pipeline(test_ds)\n",
    "    backtest_batches = apply_test_pipeline(backtest_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE and MODEL_TYPE_CHECK:\n",
    "    cnt = 5\n",
    "    for z in train_batches:\n",
    "        (baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_type) = z\n",
    "        cnt -= 1 \n",
    "        if cnt == 0: break\n",
    "    print(baseId.shape, sequence.shape, base_bb.shape, baseDateDetails.shape, mask.shape, base_label.shape, seq_type.shape)\n",
    "    sample_x = (sequence, base_bb, baseDateDetails, mask)\n",
    "    sample_y = (base_label, seq_type)\n",
    "    # print(baseId.numpy(), base_bb.numpy(), baseDateDetails.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "class PositionalEmbedding(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # if BALANCE_POS_CODE:\n",
    "        #     self.total_years = 10   # not sure if it will cover all (baseDayss - sequenceDays)\n",
    "        # else:\n",
    "        self.total_years = SEQ_YEAR_SPAN  # datetime.datetime.now().year - config['baseDate'].year + 1 + 3   # 3 extra years. ------------\n",
    "        self.total_days = 365 * self.total_years\n",
    "\n",
    "        positional_resolution = d_model\n",
    "        quotient = self.total_days / positional_resolution\n",
    "        positions = tf.constant(range(self.total_days), dtype=tf.float32)    # (total_days,). range [0, total_days)\n",
    "        fractional_pos = positions / quotient  # (total_days,). range (0, d_model)\n",
    "        half_depth = d_model/2   #\n",
    "        depths = tf.range(half_depth, dtype=tf.float32) / half_depth  # (half_depth,). range [0, 1), linear.\n",
    "        BIG = d_model * 0.8\n",
    "        depths = 1.0 / tf.pow(BIG, depths)        # (depth,). range [1, 1/BIG)\n",
    "        angle_rads = fractional_pos[:, tf.newaxis] * depths  # (total_days, half_depth,). range [dayPos, dayPos/BIG) for each day\n",
    "        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)   # Seperated sin and cos. (batch, seq_len, d_model)\n",
    "        self.total_positional_code = pos_encoding\n",
    "        return\n",
    "\n",
    "    def call(self, x, sequenceDays, baseDays, isEncoder):\n",
    "\n",
    "        if IGNORE_HISTORY: pass # x: (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "        else:\n",
    "            if BALANCE_POS_CODE:\n",
    "                # sequneceDays - (baseDays - totalDays),  baseDays - (baseDays - totalDays)\n",
    "                positions = tf.cast(baseDays - sequenceDays, dtype=tf.int32) if isEncoder else tf.cast(baseDays - baseDays, dtype=tf.int32)\n",
    "            else:\n",
    "                positions = tf.cast(self.total_days - sequenceDays, dtype=tf.int32) if isEncoder else tf.cast(self.total_days - baseDays, dtype=tf.int32)\n",
    "            positional_code = tf.gather(self.total_positional_code, positions)\n",
    "            if BALANCE_POS_CODE:\n",
    "                positional_code /= tf.math.sqrt(tf.cast(x.shape[-1], tf.float32)) ################# rather than multiply to x. This makes a balance. Not in \"Attention is all you need.\"\n",
    "            x = x + positional_code\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEmbedding(150)\n",
    "positions = tf.constant(range(pe.total_days), dtype=tf.int32)\n",
    "positions = pe.total_days - positions\n",
    "pcode = tf.gather(pe.total_positional_code, positions)\n",
    "# print(pcode)\n",
    "plt.pcolormesh(pcode.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEmbedding(150)\n",
    "positions = tf.constant(range(0, 365 * 25, 365), dtype=tf.int32)\n",
    "positions = pe.total_days - positions\n",
    "pcode = tf.gather(pe.total_positional_code, positions)\n",
    "# print(pcode)\n",
    "plt.pcolormesh(pcode.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class hParams:    \n",
    "    nDivisions = len(DIVIISONS) \n",
    "    division_embs = 4   # [0, 1, 2, 3]\n",
    "\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 4  # [0, 1, 2, 3] Maximum nGoals goals for a team in each of 1st and 2nd halfs. Extra goals will be clipped_by_value.\n",
    "    goal_embs = 4\n",
    "    nShoots = 21    # [0, ..., 20]\n",
    "    shoot_embs = 4 # for combination\n",
    "    nShootTs = 11   # [0, ..., 10]\n",
    "    shootT_embs = 4 # for combination\n",
    "    nCorners = 11   # [0, ..., 10]\n",
    "    corner_embs = 4 # for combination\n",
    "    nFauls = 21     # [0, ..., 20]\n",
    "    faul_embs = 2 # for combination\n",
    "    nYellows = 5    # [0, ..., 4]\n",
    "    yellow_embs = 2 # for combination\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "\n",
    "    # d_model DEPENDS...\n",
    "    # We want as small d_model as possible, because from which we can freely choose an actual d_model.\n",
    "    # Tests seem to indicate that larger d_model leads to training overfitting and validation underfitting.\n",
    "    \n",
    "    # Where goes ODDS_IN_DECODER ????????????????????\n",
    "\n",
    "    d_model = 1 * division_embs + 2 * team_embs     # 1 division, 2 teams\n",
    "    if ODDS_IN_ENCODER: d_model = d_model + (NUMBER_BOOKIES if FOCUS_BOOKIE is None else 1) * NUMBER_QUERIES  # + NUMBER_QUERIES odds * nBookies\n",
    "    d_encoder = d_decoder = 0\n",
    "    if EMBED_AB_COLS:\n",
    "        d_encoder = d_model + 1 * goal_embs + 1 * goal_embs + 1 * (shoot_embs + shootT_embs + corner_embs + faul_embs + yellow_embs)\n",
    "        if DECODE_BASE_DATE: d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "        else: d_decoder = d_model\n",
    "    else:   # This mode, EMBED_AB_COLS = False, gives much smaller d_moddel, maybe avoiding overfitting.\n",
    "        d_encoder = d_model + 1 * len(AB_cols)\n",
    "        if DECODE_BASE_DATE: d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "        else: d_decoder = d_model\n",
    "    d_model = max(d_encoder, d_decoder)     # (136, 118) for EMBED_AB_COLS, (126, 118) for False EMBED_AB_COLS\n",
    "    d_model += ADDITIONAL_D_MODEL               # Adjust for the model size and overfitting.\n",
    "    d_model = d_model + d_model % 2     # make it an even number.\n",
    "    print(\"d_model raw: \", d_model)\n",
    "    d_model = int( (d_model-1) / num_heads) * num_heads + num_heads  # make it a multiple of num_heads, closest to raw d_model.\n",
    "    print(\"d_model refined: \", d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(tf.keras.Model):\n",
    "    def __init__(self, hParams, isEncoder):\n",
    "        super().__init__()\n",
    "        self.isEncoder = isEncoder\n",
    "\n",
    "        # game\n",
    "        self.division_emb = tf.keras.layers.Embedding(hParams.nDivisions, hParams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        self.team_emb = tf.keras.layers.Embedding(hParams.nTeams, hParams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                # AB_cols\n",
    "                self.firstH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.secondH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.shoot_emb = tf.keras.layers.Embedding(hParams.nShoots * hParams.nShoots, hParams.shoot_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.shootT_emb = tf.keras.layers.Embedding(hParams.nShootTs * hParams.nShootTs, hParams.shootT_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.corner_emb = tf.keras.layers.Embedding(hParams.nCorners * hParams.nCorners, hParams.corner_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.faul_emb = tf.keras.layers.Embedding(hParams.nFauls * hParams.nFauls, hParams.faul_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.yellow_emb = tf.keras.layers.Embedding(hParams.nYellows * hParams.nYellows, hParams.yellow_embs, dtype=tf.float32, mask_zero=False)\n",
    "            else:\n",
    "                params = []\n",
    "                for col in AB_cols:\n",
    "                    params.append(normalization_parms[col])\n",
    "                self.AB_mormalization_params = tf.Variable(params, dtype=tf.float32, trainable=False)  # (num AB_cols=14, 3 = <mean, std, maximum>)\n",
    "\n",
    "        if not self.isEncoder:\n",
    "            if DECODE_BASE_DATE:\n",
    "                self.day_emb = tf.keras.layers.Embedding(31, 2, dtype=tf.float32, mask_zero=False)\n",
    "                self.month_emb = tf.keras.layers.Embedding(12, 2, dtype=tf.float32, mask_zero=False)\n",
    "                self.wday_emb = tf.keras.layers.Embedding(7, 2, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        if IGNORE_HISTORY: pass\n",
    "        else: self.dimensional_permution = tf.keras.layers.Dense(hParams.d_model)\n",
    "\n",
    "        self.idx_Days = BB_cols.index('Date')\n",
    "\n",
    "    def representDateDetails(self, dateDetails):\n",
    "        # dateDetails: (batch, 1, 4)\n",
    "        bYears, bMonths, bDays, bWDays = tf.split(dateDetails, [1, 1, 1, 1], axis=-1)   # All should be of (batch, seq_len = 1, 1)\n",
    "        bYears = tf.cast(bYears, dtype=tf.float32)  # (batch, seq_len = 1, 1)\n",
    "        bDays = self.day_emb(bDays)[:, :, -1]       # (batch, seq_len = 1, embs = 2)\n",
    "        bMonths = self.month_emb(bMonths)[:, :, -1] # (batch, seq_len = 1, embs = 2)\n",
    "        bWDays = self.wday_emb(bWDays)[:, :, -1]    # (batch, seq_len = 1, embs = 2)\n",
    "        # w = tf.Variable(np.math.pi / 25, dtype=tf.float32, trainable=False)    # 25 years are covered by pi or a half circle.\n",
    "        w = np.math.pi / 25\n",
    "        bYearsCos = tf.math.cos(bYears * w)\n",
    "        bYearsSin = tf.math.sin(bYears * w)\n",
    "        bYears = tf.concat([bYearsCos, bYearsSin], axis=-1)   # (batch, seq_len = 1, 1+1 = 2)\n",
    "        return bYears, bMonths, bDays, bWDays\n",
    "\n",
    "    def combined_embeddings_of_double_columns(self, emb_layer, columns, nValues):\n",
    "        # Assume emb_layer = Embedding(nValues * nValues, embs, mask_zero=False)\n",
    "        cols = tf.cast(columns, dtype=tf.int32)\n",
    "        cols = tf.clip_by_value(cols, 0, nValues-1)\n",
    "        combi = cols[:, :, 0] * nValues + cols[:, :, 1]   # (batch, seq_len, 1). [0, ..., nValues * nValues - 1]\n",
    "        combi = emb_layer(combi)\n",
    "        return combi    # (batch, seq_len, 1)\n",
    "\n",
    "    def call(self, x):\n",
    "        (sequence, base_bb, baseDateDetails, mask) = x # sob = sequence or base_bb\n",
    "        sequenceDays = sequence[:, :, self.idx_Days]  # (batch, seq_len)\n",
    "        baseDays = base_bb[:, :, self.idx_Days]   # (batch, 1)\n",
    "\n",
    "        # sequence follows BBAB, whereas base_bb follows \n",
    "        \n",
    "        # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "        if self.isEncoder:\n",
    "            # ramainder: Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols  --- total 12 fields.\n",
    "            id, div, days, teams, odds, half_goals, full_goals, shoot, shootT, corner, faul, yellow\\\n",
    "            = tf.split(sequence, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Half_Goal_cols), len(Full_Goal_cols), \\\n",
    "                                  len(Shoot_cols), len(ShootT_cols), len(Corner_cols), len(Faul_cols), len(Yellow_cols)], axis=-1)\n",
    "            # All shape of (batch, sequence, own_cols), all tf.flaot32\n",
    "        else:\n",
    "            id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "            # remainder: [] \n",
    "            # All shape of (batch, 1, own_cols), guess., all tf.float32\n",
    "\n",
    "        if FOCUS_BOOKIE is not None:\n",
    "            odds = odds[:, :, FOCUS_BOOKIE * NUMBER_QUERIES: (FOCUS_BOOKIE+1) * NUMBER_QUERIES]\n",
    "        \n",
    "        div = self.division_emb(tf.cast(div, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=1, division_embs)\n",
    "        div = tf.reshape(div, [div.shape[0], div.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=1*division_embs) --- \n",
    "        teams = self.team_emb(tf.cast(teams, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=2, team_embs)\n",
    "        teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=2*team_embs) --- \n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                first_half_goals = self.combined_embeddings_of_double_columns(self.firstH_goal_emb, half_goals, hParams.nGoals)\n",
    "                second_half_goals = self.combined_embeddings_of_double_columns(self.secondH_goal_emb, full_goals - half_goals, hParams.nGoals)\n",
    "                shoot = self.combined_embeddings_of_double_columns(self.shoot_emb, shoot, hParams.nShoots)\n",
    "                shootT = self.combined_embeddings_of_double_columns(self.shootT_emb, shootT, hParams.nShootTs)\n",
    "                corner = self.combined_embeddings_of_double_columns(self.corner_emb, corner, hParams.nCorners)\n",
    "                faul = self.combined_embeddings_of_double_columns(self.faul_emb, faul, hParams.nFauls)\n",
    "                yellow = self.combined_embeddings_of_double_columns(self.yellow_emb, yellow, hParams.nYellows)\n",
    "                if ODDS_IN_ENCODER: concat = [div, teams, odds, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow]\n",
    "                else: concat = [div, teams, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow]\n",
    "\n",
    "            else:   # normalize now\n",
    "                # AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "                AB_values = [half_goals, full_goals, shoot, shootT, corner, faul, yellow]   # all (batch, 2 cols)\n",
    "                AB_values = tf.concat(AB_values, axis=-1) # (batch, seq_len, num_AB_cols=14)\n",
    "                # self.AB_mormalization_params  # (num AB_cols=14, 3 = <mean, std, maximum>)\n",
    "                AB_values = (AB_values - self.AB_mormalization_params[:, 0]) / self.AB_mormalization_params[:, 1]\n",
    "                if ODDS_IN_ENCODER: concat = [div, teams, odds, AB_values]\n",
    "                else: concat = [div, teams, AB_values]\n",
    "        else:\n",
    "            if DECODE_BASE_DATE:\n",
    "                bYears, bMonths, bDays, bWDays = self.representDateDetails(baseDateDetails)\n",
    "                if ODDS_IN_DECODER:  concat = [div, teams, odds, bYears, bMonths, bDays, bWDays]\n",
    "                else: concat = [div, teams, bYears, bMonths, bDays, bWDays]\n",
    "            else:\n",
    "                if ODDS_IN_DECODER: concat = [div, teams, odds]\n",
    "                else: concat = [div, teams]\n",
    "\n",
    "        concat = tf.concat(concat, axis=-1)\n",
    "        assert concat.shape[-1] <= hParams.d_model        \n",
    "\n",
    "        if IGNORE_HISTORY: pass # concat: (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "        else:\n",
    "            concat = self.dimensional_permution(concat)  # (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "\n",
    "        if self.isEncoder:  mask = mask     # (batch, MAX_TOKEN, MAX_TOKEN), although (batch, 1, MAX_TOKEN) will propagate.\n",
    "        else:   mask = mask[:, 0:concat.shape[1], :]    # concat: (batch, 1, MAX_TOKEN)\n",
    "\n",
    "        return concat, mask, sequenceDays, baseDays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isEncoder = True\n",
    "if TRAIN_MODE and MODEL_TYPE_CHECK:\n",
    "    pre = Preprocess(hParams, isEncoder=isEncoder)\n",
    "    x, mask, seqDays, bDays = pre(sample_x); pre.summary()\n",
    "    pos = PositionalEmbedding(hParams.d_model)\n",
    "    x = pos(x, seqDays, bDays, isEncoder=isEncoder); pos.summary()\n",
    "    @tf.function\n",
    "    def fun(input):\n",
    "        x, mask, sequenceDays, baseDays = pre(input)\n",
    "        x = pos(x, sequenceDays, baseDays, isEncoder=isEncoder)\n",
    "        return x, mask, sequenceDays, baseDays\n",
    "    x, mask, sequenceDays, baseDays = fun(sample_x)\n",
    "    print(x.shape, mask.shape, sequenceDays.shape, baseDays.shape)\n",
    "    del pre, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isEncoder = False\n",
    "if TRAIN_MODE and MODEL_TYPE_CHECK:\n",
    "    pre = Preprocess(hParams, isEncoder=isEncoder)\n",
    "    x, mask, seqDays, bDays = pre(sample_x); pre.summary()\n",
    "    pos = PositionalEmbedding(hParams.d_model)\n",
    "    x = pos(x, seqDays, bDays, isEncoder=isEncoder); pos.summary()\n",
    "    @tf.function\n",
    "    def fun(input):\n",
    "        x, mask, sequenceDays, baseDays = pre(input)\n",
    "        x = pos(x, sequenceDays, baseDays, isEncoder=isEncoder)\n",
    "        return x, mask, sequenceDays, baseDays\n",
    "    x, mask, sequenceDays, baseDays = fun(sample_x)\n",
    "    print(x.shape, mask.shape, sequenceDays.shape, baseDays.shape)\n",
    "    del pre, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "      # tf.keras.layers.Dropout(dropout_rate)\n",
    "  ])\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v), seq_len_k == sel_len_v\n",
    "    mask: Float 0/1 tensor with shape broadcastable to (..., seq_len_q, seq_len_k). 1 surpresses the score to zero.\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  if mask is not None: scaled_attention_logits += (tf.cast(mask, dtype=tf.float32) * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, d_head)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, d_head)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, d_head)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, d_head)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, d_head)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "    # value = x, key = x, query = x\n",
    "    self_att, self_att_weights = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    self_att = self.dropout1(self_att, training=training)\n",
    "    out1 = self.layernorm1(x + self_att)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, context, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # x: (batch, target_seq_len, d_model), context.shape: (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    # value = x, key = x, query = x\n",
    "    self_att, self_att_weights = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len = 1, d_model)\n",
    "    self_att = self.dropout1(self_att, training=training)\n",
    "    self_att = self.layernorm1(self_att + x)\n",
    "\n",
    "    # value = context, key = context, query = self_att\n",
    "    cross_att, cross_att_weights = self.mha2(context, context, self_att, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    cross_att = self.dropout2(cross_att, training=training)\n",
    "    out2 = self.layernorm2(cross_att + self_att)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out, self_att_weights, cross_att_weights\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x, training, encMask):\n",
    "      x = self.dropout(x, training=training)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, training, encMask)\n",
    "      return x  # (batch_size, max_tokens, d_model)\n",
    " \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      if IGNORE_HISTORY: pass\n",
    "      else:\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, rate=dropout_rate)\n",
    "            for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x, context, training, look_ahead_mask, padding_mask):\n",
    "      if IGNORE_HISTORY: pass\n",
    "      else:\n",
    "        x = self.dropout(x, training=training)\n",
    "        for decoder_layer in self.dec_layers:\n",
    "          x, _, _  = decoder_layer(x, context, training, look_ahead_mask, padding_mask)\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      if IGNORE_HISTORY:\n",
    "        self.all_one_context = tf.ones((BATCH_SIZE, MAX_TOKENS, hParams.d_model), dtype=tf.float32) # (batch, max_tokens, d_model)\n",
    "      else:\n",
    "        self.encPreprocess = Preprocess(hParams, isEncoder=True)\n",
    "        self.decPreprocess = Preprocess(hParams, isEncoder=False)\n",
    "        self.posEmbedding = PositionalEmbedding(hParams.d_model)\n",
    "        self.encoder = Encoder(hParams, dropout_rate=dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(hParams.d_model) #-------------- to modify\n",
    "      self.decoder = Decoder(hParams, dropout_rate=dropout_rate)\n",
    "      self.one = tf.constant(1, dtype=tf.int32)\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "      # inputs = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), baseDateDetails: (batch, 1, 4), mask: (batch, max_token, max_token)\n",
    "      if IGNORE_HISTORY: \n",
    "        context = self.all_one_context\n",
    "      else:\n",
    "        x, encMask, sequenceDays, baseDays = self.encPreprocess(input) # (batch, max_tokens, d_model), (batch, max_tokens, max_tokens), (batch, max_tokens), (batch, 1)\n",
    "        x = self.posEmbedding(x, sequenceDays, baseDays, isEncoder=True) # (batch, max_tokens, d_model)\n",
    "        encMask = self.one - encMask    # NEGATE !!! forgoten for two YEARS !!!\n",
    "        encMask = encMask[:, tf.newaxis, :, :]  # newaxis: head\n",
    "        context = self.encoder(x, training, encMask)  # (batch, max_tokens, d_model). Only sequence and mask are used.\n",
    "\n",
    "      x, decMask, sequenceDays, baseDays = self.decPreprocess(input) # (batch, 1, d_model), (batch, 1, max_tokens), (batch, max_tokens), (batch, 1)\n",
    "      x = self.posEmbedding(x, sequenceDays, baseDays, isEncoder=False) # (batch, 1, d_model)\n",
    "      decMask = decMask[:, tf.newaxis, :, :]\n",
    "      # look_ahead_mask is None, which means [[0]], as there is only one position in x, so is nothing to mask when doing mha(value=x, key=x, query=x).\n",
    "      x = self.decoder(x, context, training, look_ahead_mask=None, padding_mask=decMask)  # (batch, 1, d_model).  Only base_bb, baseDateDetails, and mask are used.      \n",
    "\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE and MODEL_TYPE_CHECK:\n",
    "    sample_transformer = Transformer(hParams, dropout_rate=DROPOUT)\n",
    "    y = sample_transformer(sample_x, training=True)\n",
    "    sample_transformer.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def fun(x):\n",
    "        y = sample_transformer(x, training=False)\n",
    "        return y\n",
    "    y = fun(sample_x)\n",
    "    print(y.shape)\n",
    "    del sample_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Add_Norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, seed):\n",
    "      super().__init__()\n",
    "      self.dense =tf.keras.layers.Dense (dim, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=seed), bias_initializer=Zeros, activation='tanh')\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    def call(self, x):\n",
    "      dense = self.dense(x, training=False)\n",
    "      x = self.add([x, dense])  # x.shape[-1] == dim\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "\n",
    "Zeros = tf.keras.initializers.Zeros()\n",
    "\n",
    "# Used for earlier versions that don't allow mixing bookies.\n",
    "class Adaptor(tf.keras.Model):\n",
    "  def __init__(self, nLayers, d_main, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    # total (nLayers + nLayers) dims = 2 * nLayers dims\n",
    "    dims = [d_main] * nLayers\n",
    "    layers = [Dense_Add_Norm(dims[id], id) for id in range(len(dims))]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "    self.initial = tf.keras.layers.Dense (d_main, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=23), activation='tanh')\n",
    "    self.final = tf.keras.layers.Dense (d_output, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=23), activation='tanh')\n",
    "  def call(self, x, training=False):  # (batch, d_model)\n",
    "    x = self.initial(x)\n",
    "    x = self.seq(x)   # (batch, d_model)\n",
    "    x = self.final(x) # (batch, nBookies * NUMBER_QUERIES)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = tf.Variable([normalization_parms[col][0] for col in Odds_cols], trainable=False)\n",
    "std_variation = tf.Variable([normalization_parms[col][1] for col in Odds_cols], trainable=False)\n",
    "print(std_mean.shape, std_variation.shape)\n",
    "\n",
    "# No, _Label_cols were not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1X2(tf.keras.Model):\n",
    "    softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.nQueries = nQueries\n",
    "        self.transformer = Transformer(hParams, dropout_rate=dropout_rate)\n",
    "        #   self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "        self.nBookies = NUMBER_BOOKIES if FOCUS_BOOKIE is None else 1\n",
    "\n",
    "        if SIMPLIFY_ADAPTOR:\n",
    "            self.adaptor = tf.keras.layers.Dense(self.nBookies * self.nQueries)\n",
    "        else:\n",
    "            self.adaptor = Adaptor(ADAPTORS_LAYERS, hParams.d_model * ADAPTORS_WIDTH_FACTOR, self.nBookies * self.nQueries)\n",
    "     \n",
    "        if LOSS_TYPE == 'entropy':\n",
    "            self.categorical_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size')\n",
    "        return\n",
    "\n",
    "    def call(self, x, trainig=False):\n",
    "        x = self.transformer(x, training=trainig) # (batch, d_model)\n",
    "        output = self.adaptor(x)   # (batch, nBookies * nQueries)\n",
    "        output = tf.reshape(output, [output.shape[0], self.nQueries, -1])    # (batch, nQueries, nBookies)\n",
    "        output = tf.transpose(output, perm=[2, 0, 1])     # (nBookies, batch, nQueries)\n",
    "        if MODEL_ACTIVATION == 'softmax':\n",
    "            output = tf.nn.softmax(output)  # (nBookies, batch, nQueries)   #\n",
    "        elif MODEL_ACTIVATION == 'sigmoid':\n",
    "            output = tf.math.sigmoid(output * 5)  # the previous activation is tanh, ranging (-1, 1). Multiplier 5 will result in range (near 0, near 1)\n",
    "        elif MODEL_ACTIVATION == 'relu':\n",
    "            output = tf.nn.relu(output)\n",
    "        elif MODEL_ACTIVATION == 'open':\n",
    "            pass    # output = output\n",
    "        return output   # (nBookies, batch, nQueries)\n",
    "    \n",
    "    def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "        # ftGoals:  (batch, 2)\n",
    "        ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "        h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "        h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "        return h\n",
    "\n",
    "    def loss(self, y, output):   \n",
    "        # y = (base_label, seq_type):  ( (batch, len(Team_cols)+len(Odds_cols)), (batch,) ), \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        (base_label, seq_type) = y\n",
    "\n",
    "        if FILTER_FOR_GOOD_DATA: pass\n",
    "            # apply a filter to both output and base_label, on the batch axis.\n",
    "\n",
    "        def well(x, mu, sigma):\n",
    "            norm = tf.norm(x)\n",
    "            return tf.nn.relu(norm - sigma) * 0.5   # so this component of loss is less steep than the other component.\n",
    "            # return - tf.math.exp(- tf.math.pow((x-mu)/sigma, 2) / 2) / (sigma * tf.math.sqrt(np.pi*2))\n",
    "\n",
    "        ftGoals, odds = tf.split(base_label, [2, -1], axis=-1) # (batch, 2), (batch, len(self.bookies * self.nQueries))        if FOCUS_BOOKIE is not None:\n",
    "\n",
    "        if FOCUS_BOOKIE is not None:\n",
    "            odds = odds[:, FOCUS_BOOKIE * NUMBER_QUERIES: (FOCUS_BOOKIE+1) * NUMBER_QUERIES]\n",
    "\n",
    "        odds = tf.split(odds, [self.nQueries] * self.nBookies, axis=-1)  # [(batch, nQueries)] * self.nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (self.nBookies, batch, nQueries)\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (self.nBookies, batch, nQueries)\n",
    "\n",
    "        if LOSS_TYPE != 'entropy':\n",
    "            (stake_p) = output  # (self.nBookies, batch, nQueries)\n",
    "            # -----------------------------------------------------------------------------------------\n",
    "            # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "            # Note: Do not normalize stake_p. It can learn whether to bet or not, as well as betting direction.\n",
    "            #------------------------------------------------------------------------------------------\n",
    "            profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)    # (nBooies, batch)\n",
    "            mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "            profit_backtest_sum = tf.reduce_sum(mean_profit_by_game, axis=None)  # () \n",
    "            loss_sum = - profit_backtest_sum  # U.action.42\n",
    "\n",
    "            if MODEL_ACTIVATION == 'open':\n",
    "                bell_loss = well(stake_p, 0, 2)\n",
    "                loss += bell_loss\n",
    "\n",
    "            if not VECTOR_BETTING:\n",
    "                if STAKE_TECHNIQUE == 'independent':\n",
    "                    stake_like = stake_p    # (self.nBookies, batch, nQueries)\n",
    "                elif STAKE_TECHNIQUE == 'majority':\n",
    "                    stake_like = tf.reduce_sum(stake_p, axis=0)     # (batch, nQueries)\n",
    "                    stake_like = tf.stack([stake_like] * stake_p.shape[0], axis=0)  # (self.nBookies, batch, nQueries)\n",
    "                elif STAKE_TECHNIQUE == 'riskest':\n",
    "                    stake_like = tf.math.multiply(stake_p, odds)    # (self.nBookies, batch, nQueries)\n",
    "                    stake_like = tf.reduce_sum(stake_like, axis=0)  # (batch, nQueries)\n",
    "                    stake_like = tf.stack([stake_like] * stake_p.shape[0], axis=0)  # (self.nBookies, batch, nQueries)\n",
    "\n",
    "                one_hot_stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(stake_like).indices, tf.shape(stake_like)[-1]), axis=-2)   # one_hot stake_p, (self.nBookies, batch, nQueries)\n",
    "                profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, one_hot_stake_p), axis=-1)    # (self.nBookies, batch)\n",
    "                mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "                profit_backtest_sum = tf.reduce_sum(mean_profit_by_game, axis=None)  # ()\n",
    "\n",
    "        else:\n",
    "            probability_p = tf.transpose(output, perm=[1, 2, 0])\n",
    "            probability_p = tf.math.reduce_mean(probability_p, axis=-1) # (batch, nQueries)\n",
    "            loss_sum = self.categorical_crossentropy(happen_t, probability_p)   # reduce sum over batch size. What over the axis -1.\n",
    "\n",
    "            one_hot_stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(probability_p).indices, tf.shape(probability_p)[-1]), axis=1)   # one_hot stake_p\n",
    "            \n",
    "            profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, one_hot_stake_p), axis=-1)    # (self.nBookies, batch)\n",
    "            mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "            profit_backtest_sum = tf.reduce_sum(mean_profit_by_game, axis=None)  # ()\n",
    "    \n",
    "        return loss_sum, profit_backtest_sum, mean_profit_by_game # (), (), (batch,)  Both loss_sum and profit_backtest_sum are a sum across batch. \n",
    "\n",
    "\n",
    "def create_model_object(model_class):\n",
    "    try: model_c\n",
    "    except NameError: pass\n",
    "    else: del model_c\n",
    "    try: model_1x2\n",
    "    except NameError: pass\n",
    "    else: del model_1x2\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "    model = model_class(hParams, nQueries=NUMBER_QUERIES, dropout_rate=DROPOUT)   # Do not create a reference and return directly.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = create_model_object(Model_1X2)\n",
    "\n",
    "if TRAIN_MODE and MODEL_TYPE_CHECK:\n",
    "    output = model_1x2(sample_x, training=True)\n",
    "    @tf.function\n",
    "    def fun(x, y):\n",
    "        output = model_1x2(x, training=False)\n",
    "        loss_sum, profit_sum, profits = model_1x2.loss(y, output)\n",
    "        return output, loss_sum, profit_sum, profits\n",
    "    output, loss_sum, profit_sum, profits = fun(sample_x, sample_y)\n",
    "    print(output.shape, loss_sum.shape, profit_sum.shape, profits.shape)\n",
    "\n",
    "    model_1x2.summary()\n",
    "    # del model_1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_1x2, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.history = {'initial_profit': -float('inf'), 'loss': [], 'val_loss_0': [], 'val_profit_0': [], 'val_loss': [], 'val_profit': [], 'learning_rate': [], 'recall': [], 'precision': [], 'time_taken': [],  'gauge': [], 'backtest_reports': []}\n",
    "    def set_initial_interval_profit(self, initail_inverval_profit):\n",
    "        self.history['initial_profit'] = initail_inverval_profit\n",
    "        self.save()\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        YK_assistant.SaveJsonData(self.history, self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        # forgot to reset self.history? ---------------------- Check it.\n",
    "        self.__init__(self.filepath)\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        history = YK_assistant.LoadJsonData(self.filepath)\n",
    "        if history is not None: self.history = history\n",
    "\n",
    "    def append(self, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, gauge, backtest_report):\n",
    "        self.history['loss'].append(self.round_sig(train_loss, 4))\n",
    "        self.history['val_loss_0'].append(self.round_sig(val_loss_0, 4))\n",
    "        self.history['val_profit_0'].append(self.round_sig(val_profit_0, 4))\n",
    "        self.history['val_loss'].append(self.round_sig(val_loss, 4))\n",
    "        self.history['val_profit'].append(self.round_sig(val_profit, 4))\n",
    "        self.history['learning_rate'].append(learning_rate)\n",
    "        self.history['recall'].append(self.round_sig(recall, 4))\n",
    "        self.history['precision'].append(self.round_sig(precision, 4))\n",
    "        self.history['time_taken'].append(time_taken)\n",
    "        self.history['gauge'].append(gauge)\n",
    "        self.history['backtest_reports'].append(backtest_report)\n",
    "        self.save()\n",
    "\n",
    "    def get_zipped_history(self):\n",
    "        z = zip(self.history['loss'], self.history['val_loss_0'], self.history['val_profit_0'], self.history['val_loss'], self.history['val_profit'], self.history['learning_rate'], self.history['recall'], self.history['precision'], self.history['time_taken'])\n",
    "        return list(z)\n",
    "    # def append_backtests(self, epoch, key_a, key_b, interval_profit, backtest):\n",
    "    #     self.history['backtests'].append((epoch, key_a, key_b, interval_profit, backtest))\n",
    "    #     self.save()\n",
    "    def get_backtest_report(self, epoch):\n",
    "        return self.history['backtest_reports'][epoch]     # sure exists. epoch is selected.\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss_0'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_profit_0'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_profit'])\n",
    "        assert len(self.history['loss']) == len(self.history['recall'])\n",
    "        assert len(self.history['loss']) == len(self.history['precision'])\n",
    "        assert len(self.history['loss']) == len(self.history['learning_rate'])\n",
    "        assert len(self.history['loss']) == len(self.history['time_taken'])\n",
    "        assert len(self.history['loss']) == len(self.history['gauge'])\n",
    "        assert len(self.history['loss']) == len(self.history['backtest_reports'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_min_val_loss(self):\n",
    "        return float('inf') if self.len() <= 0 else min(self.history['val_loss'])\n",
    "    def get_max_gauge(self, epoch):\n",
    "        gauges = self.history['gauge']\n",
    "        return -float('inf') if (len(gauges) <= 0 or epoch <= 0) else max(gauges[:epoch])\n",
    "    def replace_gauge(self, epoch, gauge):\n",
    "        self.history['gauge'][epoch] = gauge;   self.save()\n",
    "    def show(self, ax, show_val_0=True):\n",
    "        ax.set_title(MODEL_ID + \": training history: max gauge = \" + str(max(self.history['gauge']) if len(self.history['gauge']) > 0 else 0))\n",
    "\n",
    "        ax.plot(self.history['loss'], label='train_loss')\n",
    "        if show_val_0: ax.plot(self.history['val_loss_0'], label='GET_VAL_LOSS_0')\n",
    "        ax.plot(self.history['val_loss'], label='val_loss')\n",
    "        ax.plot([-v for v in self.history['val_loss']], label='vector_profit')\n",
    "        ax.plot(self.history['val_profit'], label='scalar_profit')\n",
    "        \n",
    "        ax.legend(loc='lower left')\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('loss or profit')\n",
    "        ax.set_xlabel('epoch', loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recall():\n",
    "  def __init__(self, **kwargs):\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):    # (batch,)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    labeled_positives = tf.math.reduce_sum(label, axis=None)\n",
    "    recall = hit_positives / (labeled_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, **kwargs):\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    predicted_positives = tf.math.reduce_sum(pred, axis=None)\n",
    "    precision = hit_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function    # Removing this decoration leads to GPU OOM!!!\n",
    "def train_step(model, optimizer, x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(x, training=True)  # (nBookies, batch, nQueries)\n",
    "        loss_sum, _, _ = model.loss(y, outputs)    # (), (), (batch,)\n",
    "\n",
    "    grads = tape.gradient(loss_sum, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    del tape    # new\n",
    "    return loss_sum  # (), (batch,)\n",
    "\n",
    "@tf.function\n",
    "def find_loss_for_dataset_step(model, x, y):\n",
    "    outputs = model(x, training=False)  # (nBookies, batch, nQueries)\n",
    "    loss_sum, profit_sum, profits = model.loss(y, outputs)    # (), (), (batch,)\n",
    "    return loss_sum, profit_sum, profits #, profits\n",
    "\n",
    "def find_loss_for_dataset(model, ds_batches):\n",
    "    prev_nSum = new_nSum = 0\n",
    "    avg_loss = tf.Variable(0.0, dtype=tf.float32, trainable=False) \n",
    "    avg_profit = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "    total_profits = tf.Variable([0.0], dtype=tf.float32, trainable=False)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), y) in enumerate(ds_batches):\n",
    "        (base_label, seq_type) = y\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        loss_sum, profit_sum, profits = find_loss_for_dataset_step(model, x, y)\n",
    "        batch_size = baseId.shape[0]\n",
    "        new_nSum += batch_size  # += baseId.shape[0], given sum of loss and profit. Ignore that the last batch might be smaller than others.\n",
    "        avg_loss = avg_loss * prev_nSum / new_nSum + loss_sum / new_nSum\n",
    "        avg_profit = avg_profit * prev_nSum / new_nSum + profit_sum / new_nSum\n",
    "        prev_nSum = new_nSum\n",
    "        total_profits = tf.concat([total_profits, profits], axis=-1)\n",
    "    return avg_loss, avg_profit, total_profits    # agerage loss and backtest_profit per game\n",
    "\n",
    "@tf.function\n",
    "def find_output_for_dataset_step(model, x):\n",
    "    outputs = model(x, training=False)  # (nBookies, batch, nQueries)\n",
    "    return outputs\n",
    "\n",
    "def find_output_for_dataset(model, ds_batches):\n",
    "    prediction = {}\n",
    "\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), _) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        stake_p = find_output_for_dataset_step(model, x)    # (nBookies, batch, nQueries)\n",
    "        one_hot_stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(stake_p).indices, tf.shape(stake_p)[-1]), axis=-2)   # one_hot stake_p, (nBookies, batch, nQueries)\n",
    "        one_hot_stake_p = tf.transpose(one_hot_stake_p, perm=[1, 0, 2])   # (batch, nBookies, nQueries)\n",
    "        one_hot_stake_p = tf.reshape(one_hot_stake_p, [one_hot_stake_p.shape[0], -1])   # (batch, nBookies * nQueries)\n",
    "\n",
    "        baseId = baseId.numpy()     # (batch,)\n",
    "        one_hot_stake_p = one_hot_stake_p.numpy()   # (batch, nBookies * nQueries)\n",
    "        prediction = prediction | { baseId[b] : list(one_hot_stake_p[b].astype(np.int32)) for b in range(baseId.shape[0]) }\n",
    "        # output = tf.reshape(output, [output.shape[0], self.nQueries, -1])    # (batch, nQueries, nBookies)\n",
    "        # output = tf.transpose(output, perm=[2, 0, 1])     # (nBookies, batch, nQueries)       \n",
    "    return prediction\n",
    "\n",
    "class Adam_exponential(tf.keras.optimizers.Adam):\n",
    "    def __init__(self, initial_step, starting_rate, ex_step, ex_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-12):\n",
    "        self.step = tf.Variable(initial_step, dtype=tf.float32, trainable=False)\n",
    "        learning_rate = tf.compat.v1.train.exponential_decay(starting_rate, self.step, ex_step, ex_rate/starting_rate, staircase=False)\n",
    "        super().__init__(learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
    "\n",
    "# @tf.function\n",
    "def find_recall_precision(model, ds_batches):\n",
    "    recall_object.reset(); precision_object.reset()\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_type)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        pred = model(x, training=False)\n",
    "        pred = tf.cast(pred > 0.5, dtype=tf.int32)\n",
    "        # Wrong. transorm base_label to label. It has odds too.   #-------------------------------------------------------- Wrong\n",
    "        recall_object.update(base_label, pred); precision_object.update(base_label, pred)\n",
    "    return recall_object.result(), precision_object.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sub_epoch(epoch, batch_id_seen, sub_loss, epoch_loss, samples_seen, learning_rate):\n",
    "    print(\"epoch: {}, batchId_seen: {}, sub_loss: {:.6}, epoch_loss: {:.6f}, samples_seen: {}, learning_rate: {:.5e}                  \".\n",
    "            format(epoch, batch_id_seen, float(sub_loss), float(epoch_loss), samples_seen, learning_rate), end='\\r')\n",
    "\n",
    "# conclude_train_epoch(-1, 0, 0, 0, 0, playback=False)\n",
    "def conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, playback=False, gaugeTerm='val_loss'):\n",
    "    global focus_interval_id, focus_scores, focus_back, focus_valid, focus_test\n",
    "    print(\"epoch: {}, train_loss: {:.5f}, val_loss_0: {:.5f}, val_profit_0: {:.5f}, val_loss: {:.5f}, val_profit: {:.5f}, learning_rate: {:.5e}, recall/precision: {:.4f}/{:.4f}, time_taken: {:.1f} m\".format(epoch, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken))\n",
    "\n",
    "    backtest_report = ()\n",
    "    if gaugeTerm == 'val_loss': gauge = - val_loss\n",
    "    elif gaugeTerm == 'recall': gauge = recall\n",
    "    elif gaugeTerm == 'precision': gauge = precision\n",
    "    elif gaugeTerm == 'val_profit': gauge = val_profit\n",
    "\n",
    "    # backtest_report, gauge = run_backtest(epoch, model, history, playback)   #-------------------------------------------------\n",
    "\n",
    "    upgraded = False\n",
    "    #-------------------- get interval scores from arguments, create the best checkpoint if we have a highest ever score.\n",
    "    if gauge > history.get_max_gauge(epoch):\n",
    "        upgraded = True\n",
    "        # focus_interval_id, focus_scores, focus_back, focus_valid, focus_test = bets_interval_id, interval_scores, best_interval_back, best_interval_valid, best_interval_test\n",
    "        if not playback: model.save_weights(checkpointPathBest)\n",
    "        print(\"---------\")\n",
    "    if playback: history.replace_gauge(epoch, gauge)\n",
    "\n",
    "    #--------------------- Save, finally\n",
    "    if not playback:\n",
    "        model.save_weights(checkpointPath)\n",
    "        # self, train_loss, val_loss, learning_rate, recall, precision, time_taken, gauge, backtest_report\n",
    "        history.append(train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, gauge, backtest_report)\n",
    "\n",
    "    return upgraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join(checkpoint_folder_path, MODEL_ID + '_weights')\n",
    "checkpointPathBest = os.path.join(checkpoint_folder_path, MODEL_ID + '_weights_best')\n",
    "historyPath = os.path.join(checkpoint_folder_path, MODEL_ID + '_history.json')\n",
    "history = history_class(historyPath); history.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "history.show(ax, show_val_0=GET_VAL_LOSS_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = create_model_object(Model_1X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken in history.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='val_profit')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def train(epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False, gaugeTerm='val_loss', get_val_loss_0=False):\n",
    "    epochs = epochs; seen_upgrades = 0\n",
    "    for epoch in range(history.len(), history.len() + epochs):\n",
    "        start_time = time.time()\n",
    "        optimizer.step.assign(history.len()); learning_rate = optimizer.lr.numpy()\n",
    "        prev_nSum = new_nSum = 0; prev_mSum = new_mSum = 0; epoch_loss = 0.; sub_loss = 0.\n",
    "        samples_seen = 0 # sub_loss = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "        set_seed(epoch)\n",
    "        train_batches = apply_train_pipeline(train_ds)      # This is REQUIRED to reshuffle the data at each iteration. Tested.\n",
    "        for batch_id, ((baseId, sequence, base_bb, baseDateDetails, mask), y) in enumerate(train_batches):    # reshuffle at each iteration - NO!\n",
    "            x = (sequence, base_bb, baseDateDetails, mask)\n",
    "            samples_seen += sequence.shape[0]\n",
    "            loss_sum = train_step(model, optimizer, x, y)  # ()\n",
    "\n",
    "            batch_size = baseId.shape[0]\n",
    "            new_nSum += batch_size; new_mSum += batch_size\n",
    "            sub_loss = sub_loss * prev_nSum / new_nSum + loss_sum / new_nSum\n",
    "            epoch_loss = epoch_loss * prev_mSum / new_mSum + loss_sum / new_mSum\n",
    "            prev_nSum = new_nSum; prev_mSum = new_mSum\n",
    "\n",
    "            if batch_id % int(100/BATCH_SIZE) == 0:\n",
    "                # optimizer.step.assign(history.len()); learning_rate = optimizer.lr.numpy()  #----------------------------------------------------\n",
    "                show_sub_epoch(epoch, batch_id, sub_loss, epoch_loss, samples_seen, learning_rate); prev_nSum = new_nSum = 0; sub_loss = 0.0\n",
    "\n",
    "        show_sub_epoch(epoch, batch_id, sub_loss, epoch_loss, samples_seen, learning_rate)  # closing show\n",
    "        val_loss_0 = val_profit_0 = 0.0\n",
    "        if get_val_loss_0: val_loss_0, val_profit_0, _ = find_loss_for_dataset(model, train_batches)\n",
    "        val_loss, val_profit, _ = find_loss_for_dataset(model, valid_batches)\n",
    "\n",
    "        recall = precision = -1.0\n",
    "        if get_f1: recall, precision = find_recall_precision(model, valid_batches); recall = recall.numpy(); precision = precision.numpy()\n",
    "        \n",
    "        # epoch, model, history, checkpointPath, checkpointPathBest, epoch_loss, val_loss, learning_rate, recall, precision, time_taken, playback=False\n",
    "        upgraded = conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, float(epoch_loss), float(val_loss_0), float(val_profit_0), float(val_loss), float(val_profit), learning_rate, recall, precision, (time.time()-start_time)/60, playback=False, gaugeTerm=gaugeTerm)\n",
    "        if upgraded: seen_upgrades += 1\n",
    "        if seen_upgrades >= nUpgrades: break\n",
    "\n",
    "    return seen_upgrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == True and OPERATION == 'PRETRAIN': # model_1x2 is trained to find profitable staking vectors for games, although average profitability is far below zero.\n",
    "    try:    model_1x2.load_weights(checkpointPath).expect_partial(); print(\"model_1x2 loaded its previous checkpoint.\")\n",
    "    except: print(\"model_1x2 loaded its initial weights.\")    # The raw model_1x2 itself is the starting point.\n",
    "    optimizer = Adam_exponential(history.len(), STARTING_LEARNING_RATE, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.99, epsilon=1e-12)\n",
    "    # Goal: get checkpointPathBest checkpoint, which is used as the starting checkpoint of both TRAIN_C and FINETUNE.\n",
    "    nEpochs = 300; wanted_upgrades = 60\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_1x2, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1 = False, gaugeTerm='val_profit', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "    \n",
    "    OPERATION = 'TRAIN_C'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'BACKTEST':\n",
    "    startDate, endDate = YK_assistant.find_first_start_and_end_date(valid_ds)\n",
    "    valid_days = (endDate-startDate).days + 1\n",
    "    model_1x2.load_weights(checkpointPathBest)\n",
    "    avg_loss, avg_profit, profits = find_loss_for_dataset(model_1x2, valid_batches)\n",
    "    profits = list(profits.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'BACKTEST':\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "    plt.plot(profits); plt.ylim([-1.5, 5])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Model {}: Profit Per Game, {} major 4 football leagues, spaning {} days, betting 1.0 on each game, Agerage: {:.4f}\"\n",
    "              .format(MODEL_ID, COUNTRY, valid_days, sum(profits)/len(profits)))\n",
    "    plt.xlabel('game'); plt.ylabel('profit')\n",
    "    plt.show()\n",
    "\n",
    "    balance_history, max_balance, linear_deepest_canyon = YK_assistant.linear_strategy(profits)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "    plt.title(\"Model {}: Accumulated Profits, {} major 4 football leagues, spaning {} days, betting 1.0 on each game. DeepestCanyon: {}\"\n",
    "              .format(MODEL_ID, COUNTRY, valid_days, int(linear_deepest_canyon)+1))\n",
    "    plt.plot(balance_history)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('game'); plt.ylabel('accumulated profit')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'BACKTEST':\n",
    "    safety = (int(linear_deepest_canyon) + 1) * 2\n",
    "    balance_history, max_balance, deepest_canyon = YK_assistant.exponential_strategy(profits, safety)\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "    plt.title(\"Model {}: Accumulated Profits, {} major 4 football leagues, spaning {} days, betting a {}-th on each game. DeepestCanyon: {}\"\n",
    "            .format(MODEL_ID, COUNTRY, valid_days, safety, int(deepest_canyon)+1))\n",
    "    plt.plot(balance_history)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('game'); plt.ylabel('accumulated profit')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "    plt.title(\"Zooming into the first 500 games\")\n",
    "    plt.plot(balance_history[:500])\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('game'); plt.ylabel('accumulated profit')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_or_same_sheet = 'same'     # 'next', 'same'\n",
    "\n",
    "if not TRAIN_MODE:      # just sketch\n",
    "    bookie_dict = YK_assistant.find_latest_bookie_dictionary(countryTheme_folder_path, DIVIISONS)\n",
    "    bookie_list = list(bookie_dict.keys())\n",
    "    \n",
    "    filename = MODEL_ID + '.xlsx'; sheetname = next_or_same_sheet\n",
    "    input_columns, sheetName = YK_assistant.format_inference_input_excel_sheet(history_folder, filename, sheetname, bookie_list)\n",
    "    print(input_columns)\n",
    "    print(sheetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODE:      # just sketch\n",
    "    consistent, check_report = YK_assistant.check_for_consistency_input_excel_sheet(countryTheme_folder_path, history_folder, filename, sheetName, input_columns, tokenizer_team, bookie_list, DIVIISONS)\n",
    "    print('Consistent: ', consistent)\n",
    "    print(check_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "if not TRAIN_MODE: # and consistent:\n",
    "    df_bbab_to_predict = YK_assistant.build_bbab_dataframe(history_folder, filename, sheetName, BB_cols, AB_cols, bookie_dict)\n",
    "\n",
    "    YK_assistant.assign_seasonal_filenames(gameHistory_folder_path)\n",
    "    df_train, df_bbab_not_in_train = YK_assistant.CREATE_DADAFRAMES_v2(gameHistory_folder_path, countryTheme_folder_path, Required_Non_Odds_cols, NUMBER_BOOKIES, oddsGroupsToExclude = BOOKIE_TO_EXCLUDE, preferedOrder = PREFERED_ORDER, train_mode = TRAIN_MODE, skip=True)\n",
    "    print(\"df_train: \", df_train.shape)\n",
    "    print(\"df_bbab_not_in_train: \", df_bbab_not_in_train.shape)\n",
    "\n",
    "    df_sequence = pd.concat([df_train, df_bbab_not_in_train])   # No duplicates. Maximized bbab dataframe.\n",
    "    df_base = df_bbab_to_predict                                  # Minimized\n",
    "    # Do NOT save, because the 'id' column for df_bb_to_predict is temporary and arbitrary.\n",
    "    predict_map = YK_assistant.CREATE_MAP_v2(map_folder_path, map_filename_base, HISTORY_LEN, df_sequence, df_base, year_span=SEQ_YEAR_SPAN, testcount=-1, to_save=False)\n",
    "    print('predict_map', predict_map)\n",
    "\n",
    "    df_sequence = pd.concat([df_train, df_bbab_not_in_train])   # No duplicates. Maximized bbab dataframe.\n",
    "    df_base = df_bbab_to_predict; existing_ds = None; total_map = predict_map                                     # Minimized\n",
    "    # Let the function use a dummy Y, as df_base has not Y's.\n",
    "    predict_ds = CREATE_DATASET(df_sequence, df_base, existing_ds, total_map, tokenizer_team, normalization_parms)  # defines, not makes, predict_ds\n",
    "    predict_batches = apply_test_pipeline(predict_ds)\n",
    "\n",
    "    model_1x2.load_weights(checkpointPathBest)\n",
    "    prediction = find_output_for_dataset(model_1x2, predict_batches)\n",
    "\n",
    "    YK_assistant.create_output_excel_sheet(history_folder, filename, sheetName, prediction, df_bbab_to_predict, bookie_dict, AB_cols, NUMBER_QUERIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
