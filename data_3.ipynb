{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "def ensure_excelfile_exists(filePath) :\n",
    "    try :\n",
    "        # Make sure file exists.\n",
    "        if os.path.exists( filePath ) is True :\n",
    "            workbook = openpyxl.load_workbook( filePath )\n",
    "        else :\n",
    "            workbook = openpyxl.Workbook()\n",
    "            workbook.save( filePath )\n",
    "            # workbook = openpyxl.load_workbook( filePath )\n",
    "\n",
    "        # # Make sure a sheet exists for this session.\n",
    "        # if sheetName is not None and sheetName not in workbook.sheetnames :\n",
    "        #     workbook.create_sheet( title = sheetName )\n",
    "\n",
    "        # workbook.save( filePath )\n",
    "\n",
    "    except :\n",
    "        raise Exception( \"Failed ensureing Excel file exists: {}\".format(filePath) )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================== create an empty excel sheet to edit bb_to_predict into.\n",
    "import os\n",
    "\n",
    "if not False:\n",
    "\n",
    "    import pandas as pd\n",
    "    Odds_cols = []\n",
    "    for b in range(5):\n",
    "        Odds_cols += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']\n",
    "\n",
    "    # input_columns = {'no': pd.Series(dtype='int'), 'year': pd.Series(dtype='int'), 'month': pd.Series(dtype='int'), 'day': pd.Series(dtype='int'),\n",
    "    #     'Div': pd.Series(dtype='str'), 'HomeTeam': pd.Series(dtype='str'), 'AwayTeam': pd.Series(dtype='str')}\n",
    "    input_columns = {'id': [], 'year': [], 'month': [], 'day': [], 'hour': [], 'minute': [], 'Div': [], 'HomeTeam': [], 'AwayTeam': []}\n",
    "    input_columns = input_columns | {col: pd.Series(dtype='float') for col in Odds_cols}\n",
    "    df = pd.DataFrame(input_columns)\n",
    "\n",
    "    sheet = '6'\n",
    "    filePath = os.path.join(\".\", \"history\", \"test2.xlsx\")\n",
    "    ensure_excelfile_exists(filePath)\n",
    "    with pd.ExcelWriter(filePath, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "lists = [[1,2], [3,4]]\n",
    "interleave = [val for tup in zip(*lists) for val in tup]\n",
    "print(interleave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Some columns are missing: no\n",
      "Some Div values are wrong: [23]\n",
      "Some odds are too small: rows [0] columns HDA3H\n",
      "Some odds are too small: rows [0] columns HDA3D\n",
      "Some odds group are too large: odds group ['HDA1H', 'HDA1D', 'HDA1A']\n",
      "Some odds group are too small: odds group ['HDA2H', 'HDA2D', 'HDA2A']\n",
      "Some odds group are too small: odds group ['HDA3H', 'HDA3D', 'HDA3A']\n",
      "Some odds group are too small: odds group ['HDA4H', 'HDA4D', 'HDA4A']\n",
      "\n",
      "        id  year  month  day  hour  minute  Div  HomeTeam  AwayTeam  HDA0H  \\\n",
      "0  2000000  2025      4   12    12      30   23         1         1    2.6   \n",
      "\n",
      "   ...  HDA1A  HDA2H  HDA2D  HDA2A  HDA3H  HDA3D  HDA3A  HDA4H  HDA4D  HDA4A  \n",
      "0  ...      9      2      2      2      1      1      2      2      2      2  \n",
      "\n",
      "[1 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "import YG_assistant\n",
    "\n",
    "if not False:\n",
    "    df = pd.read_excel(filePath, sheet_name=sheet)\n",
    "    if 'Unnamed: 0' in list(df.columns): df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    # print(df)\n",
    "\n",
    "    check_report = \"\"\n",
    "    consistent = True\n",
    "    # Check for consistency\n",
    "    delta = list(set(list(input_columns.keys())) - set(df.columns))\n",
    "    if len(delta) > 0: \n",
    "        consistent = False\n",
    "        check_report += \"Some columns are missing: {}\\n\".format(\",\".join(delta))\n",
    "    if not (df.shape[0] > 0): \n",
    "        consistent = False\n",
    "        check_report += \"There are no data rows. \\n\"\n",
    "    if df.isnull().any().any():\n",
    "        consistent = False\n",
    "        check_report += \"There are some NaN values. \\n\"\n",
    "\n",
    "    if set(['year', 'month', 'day', 'hour', 'minute']) <= set(df.columns):\n",
    "        try:\n",
    "            datetime_series = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']], utc=True)\n",
    "            now = datetime.now(pytz.utc)\n",
    "            delta_hours = [(dt - now).total_seconds()/3600 for dt in datetime_series]\n",
    "            negatives = [id for id in range(len(delta_hours)) if delta_hours[id] < 0]\n",
    "            if len(negatives) > 0:\n",
    "                consistent = False\n",
    "                check_report += \"Some datetimes are past: indices = {}\\n\".format(\",\".join([str(id) for id in negatives]))\n",
    "            aheads = [id for id in range(len(delta_hours)) if delta_hours[id] > 24]\n",
    "            if max(delta_hours) > 24:\n",
    "                consistent = False\n",
    "                check_report += \"Some datetimes are not within 24 hours: indices = {}\\n\".format(\",\".join([str(id) for id in aheads]))\n",
    "        except:\n",
    "            consistent = False\n",
    "            check_report += \"Some datetime columns have invalid data types or value range. \\n\"\n",
    "\n",
    "    homes = list(df['HomeTeam']); aways = list(df['AwayTeam'])\n",
    "    lists = [homes, aways]\n",
    "    teamList = [val for tup in zip(*lists) for val in tup]\n",
    "    # teams, candidates, success = YG_assistant.convert_to_token_with_candidates(tokenizer, teamList, candi_count=3)\n",
    "    # if success in False:\n",
    "    #     consistent = False\n",
    "    #     check_report += \"Mis-spelled teams and their candidates comes below: \\n\"\n",
    "    #     for (spell, candis) in candidates.items():\n",
    "    #         check_report += \"{}: {}\".format(spell, candis)\n",
    "\n",
    "    DIVS = ['E0', 'E1', 'E2', 'E3']\n",
    "    delta = list(set(df['Div']) - set(DIVS))\n",
    "    if len(delta) > 0:\n",
    "        consistent = False\n",
    "        check_report += \"Some Div values are wrong: {}\\n\".format(delta)\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    NUMBER_BOOKIES = 5\n",
    "    for col in Odds_cols:\n",
    "        odds = np.array(df[col])\n",
    "        condition = odds <= 1.0\n",
    "        if condition.any():\n",
    "            consistent = False\n",
    "            check_report += \"Some odds are too small: rows {} columns {}\\n\".format(list(np.where(condition)[0]), col)\n",
    "        condition = odds > 60.0\n",
    "        if (condition).any():\n",
    "            consistent = False\n",
    "            check_report += \"Some odds are too large: rows {} columns {}\\n\".format(list(np.where(condition)[0]), col)\n",
    "\n",
    "    for b in range(NUMBER_BOOKIES):\n",
    "        cols = ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']\n",
    "        odds = np.array(df[cols])\n",
    "        condition = np.sum(1/odds, axis=-1) > 1.2\n",
    "        if condition:\n",
    "            consistent = False\n",
    "            check_report += \"Some odds group are too small: odds group {}\\n\".format(cols)\n",
    "        condition = np.sum(1/odds, axis=-1) < 0.85\n",
    "        if condition:\n",
    "            consistent = False\n",
    "            check_report += \"Some odds group are too large: odds group {}\\n\".format(cols)\n",
    "\n",
    "    print(consistent)\n",
    "    print(check_report)\n",
    "\n",
    "    # if consistent:  # convert to bbab\n",
    "    start = 1000000 * 2\n",
    "    df['id'] = list(range(start, start + df.shape[0]))\n",
    "    print(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-11\n",
      "2025-04-11\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().replace(tzinfo=pytz.utc)\n",
    "print(now.date())\n",
    "\n",
    "print(datetime.now(pytz.utc).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2  col3\n",
      "0     1     1     1\n",
      "1     2     2     2\n",
      "2     3     5     3\n",
      "   col1  col2  col3\n",
      "0     1     1     1\n",
      "1     2     2     2\n",
      "2     3     5     3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { 'col1': [1,1,3], 'col2': [1,1,5], 'col3': [1,2,3] }\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "df = df.drop_duplicates(subset=['col1', 'col2'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "<generator object generator at 0x00000205F78CC040>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generator(aa):\n",
    "    for a in aa:\n",
    "        if aa.index(a) < 4:\n",
    "            yield a\n",
    "        else:\n",
    "            aa = [a + 5 for a in aa]\n",
    "            yield generator(aa)\n",
    "\n",
    "bb = generator([1,2,3,4,5])\n",
    "\n",
    "for b in bb:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: (22, 34)}\n"
     ]
    }
   ],
   "source": [
    "d = { 1: (11, 12), 2: (22, 34) }\n",
    "f = { id: value for (id, value) in d.items() if id > 1}\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 3, 4],\n",
       "       [6, 8, 9]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([[1,2,3,4,5], [6,7,8,9,0]])\n",
    "tf.gather(a, [0, 2,3], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(5, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1 0 0 0]\n",
      " [1 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(5, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 1 1 0 0]\n",
      " [1 1 1 0 0]\n",
      " [1 1 1 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]], shape=(5, 5), dtype=int32)\n",
      "(3, 5, 5)\n",
      "tf.Tensor(\n",
      "[[[1 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]]\n",
      "\n",
      " [[1 1 0 0 0]\n",
      "  [1 1 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]]\n",
      "\n",
      " [[1 1 1 0 0]\n",
      "  [1 1 1 0 0]\n",
      "  [1 1 1 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]]], shape=(3, 5, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "len = tf.constant([1,2,3], dtype=tf.int32)\n",
    "\n",
    "total_mask = []\n",
    "for s in len:\n",
    "    mask = tf.concat([tf.ones(s, dtype=tf.int32), tf.zeros(5-s, dtype=tf.int32)], axis=-1)\n",
    "    mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]\n",
    "    total_mask.append(mask)\n",
    "    print(mask)\n",
    "total_mask = tf.stack(total_mask, axis=0)\n",
    "print(total_mask.shape)\n",
    "print(total_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col3\n",
      "0   1.0   1.0\n",
      "1   2.0   2.0\n",
      "2   3.0   NaN\n",
      "3   4.0   4.0\n",
      "4   NaN   NaN\n",
      "0    1.0\n",
      "1    2.0\n",
      "2    3.0\n",
      "3    4.0\n",
      "Name: col1, dtype: float64\n",
      "   col1  col2  col3\n",
      "0   1.0   1.0   1.0\n",
      "1   2.0   NaN   2.0\n",
      "2   3.0   3.0   NaN\n",
      "3   4.0   4.0   4.0\n",
      "4  -5.0   5.0   NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = { 'col1': [1, 2, 3, 4, np.nan], 'col2': [1, np.nan, 3, 4, 5], 'col3': [1, 2, np.nan, 4, np.nan] }\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df2 = df.loc[:, ['col1', 'col3']]\n",
    "print(df2)\n",
    "array = df2['col1'].dropna()\n",
    "print(array)\n",
    "\n",
    "df2.loc[:, 'col1'] = -3\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # if pd.isnull(row['col1']) or pd.isnull(row['col2']):\n",
    "    #     print(row['col1'], row['col2'])\n",
    "    if pd.isnull( row['col1'] ):\n",
    "        row['col1'] = -5\n",
    "\n",
    "print(df)\n",
    "# df.loc[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "COUNTRY = 'England'\n",
    "THEME = 'test_theme'\n",
    "CountryThemeFolderPath = \"./data/football-data-co-uk/\" + COUNTRY + '/' + THEME\n",
    "os.makedirs(CountryThemeFolderPath, exist_ok=True)\n",
    "os.makedirs(os.path.join(CountryThemeFolderPath, '_id_map'), exist_ok=True)\n",
    "os.makedirs(os.path.join(CountryThemeFolderPath, '_dataaset'), exist_ok=True)\n",
    "os.makedirs(os.path.join(CountryThemeFolderPath, '_checkpoint'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "gaierror",
     "evalue": "[Errno 11001] getaddrinfo failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13020\\2026272915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpayload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/v3/football/fixtures/head-to-head/{team_id_1}/{team_id2}?api_token=YOUR_TOKEN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                 encode_chunked=False):\n\u001b[0;32m   1284\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1329\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\\r\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m             \u001b[1;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    944\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         self.sock = self._create_connection(\n\u001b[0m\u001b[0;32m    947\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Might fail in OSs that don't implement TCP_NODELAY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maddress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[0msock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mike\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[1;31m# and socket type values to enum constants.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         addrlist.append((_intenum_converter(af, AddressFamily),\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"api.sportmonks.com\")\n",
    "payload = ''\n",
    "headers = {}\n",
    "conn.request(\"GET\", \"/v3/football/fixtures/head-to-head/{team_id_1}/{team_id2}?api_token=YOUR_TOKEN\", payload, headers)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "print(data.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the most re version of TensorFlow to use the improved\n",
    "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
    "# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
    "# !pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
    "# !pip install protobuf~=3.20.3\n",
    "# !pip install -q tensorflow_datasets\n",
    "# !pip install -q -U tensorflow-text tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  no name 'resource'?\n",
    "#  pip3 uninstall tensorflow_datasets; pip3 install tensorflow_datasets==4.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import data_helpers\n",
    "import data_generator\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_helpers.read_large_excel(config['data'], \"contest_total_data\")\n",
    "df = data_helpers.improve_contest_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = list(set(list(df['teams_home_team_id']) + list(df['teams_away_team_id'])))\n",
    "teams_string = [str(team) for team in teams]\n",
    "teams_text = \" \".join(teams_string)\n",
    "\n",
    "corpus_file = \"./data/tokenizers/team_ids_text.txt\"\n",
    "f = open(corpus_file, \"w\", encoding=\"utf-8\")\n",
    "f.write(teams_text)\n",
    "f.close()\n",
    "\n",
    "corpus_files = [corpus_file]\n",
    "unknown_token = config['unknown_token']\n",
    "special_tokens = [unknown_token] + [\"[HOME]\", \"[AWAY]\"]\n",
    "vocab_size = len(teams_string) + len(special_tokens)\n",
    "\n",
    "tokenizer_team = data_generator.createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_team.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer_team.encode(\"1 2 3 what? 333 -5 4)e_t grw0 1234 [HOME] [AWAY]\")\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_team.decode([3, 2046, 2921, 0, 3105, 0, 0, 0, 804, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "countries_string = list(set(list(df['league_country']) + list(df['home_team_country'])+ list(df['away_team_country'])))\n",
    "# print(countries_string.index('ViÃ° MargÃ¡ir'))\n",
    "countries_string = [re.sub(r\"\\s\", \"_\", item) for item in countries_string]    # replace spaces with a '_'\n",
    "# print(countries_string.index('ViÃ°_MargÃ¡ir'))\n",
    "# teams_string = [str(team) for team in teams]\n",
    "countries_text = \" \".join(countries_string)\n",
    "\n",
    "corpus_file = \"./data/tokenizers/country_ids_text.txt\"\n",
    "f = open(corpus_file, \"w\", encoding=\"utf-8\")\n",
    "f.write(countries_text)\n",
    "f.close()\n",
    "\n",
    "corpus_files = [corpus_file]\n",
    "unknown_token = config['unknown_token']\n",
    "special_tokens = [unknown_token] + [\"[LEAGUE_COUNTRY]\", \"[HOME_COUNTRY]\", \"[AWAY_COUNTRY]\"]\n",
    "vocab_size = len(countries_string) + len(special_tokens)\n",
    "\n",
    "tokenizer_country = data_generator.createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['Argentina', 'Italy', 'WHAT', 'France', 'USA', 'Canada', '[UNK]', 'Unknown', '[AWAY_COUNTRY]', 'Chinese Taipei', 'ViÃ° MargÃ¡ir']\n",
    "countries = [re.sub(r\"\\s\", \"_\", item) for item in countries]\n",
    "print(countries)\n",
    "countries = \" \".join(countries)\n",
    "encoding = tokenizer_country.encode(countries)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_country.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ids-5.0-0.01-15.0-0.9-0.8-10-0.000005-26.64-32601-15'\n",
    "(a_conductance_search, b_conductance_search, conductance365, \n",
    "        conductanceMedTeam, ids_max, minConductanceStep, hours, nDataPoints, maxLen) \\\n",
    "= data_helpers.filename_to_dataParams(filename)\n",
    "path = os.path.join('./data', filename + '.json')\n",
    "fixture_id_to_ids = data_helpers.LoadJsonData(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for ids in fixture_id_to_ids.values()]\n",
    "plt.hist(lengths, np.linspace(0, 200, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "for elem in dataset:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_cols = ['teams_home_team_id', 'teams_away_team_id']\n",
    "dummy_pair = \"[HOME] [AWAY]\"\n",
    "country_cols = ['league_country', 'home_team_country', 'away_team_country']\n",
    "dummy_countries = \"[LEAGUE_COUNTRY] [HOME_COUNTRY] [AWAY_COUNTRY]\"\n",
    "odds_cols = ['winning_odds_home', 'winning_odds_draw', 'winning_odds_away']\n",
    "dummy_wining_percent = (80, 10, 10)         \n",
    "dummy_odds = [\n",
    "    data_helpers.get_odds(dummy_wining_percent[0]/100, config['bookie_profit_percent']),\n",
    "    data_helpers.get_odds(dummy_wining_percent[1]/100, config['bookie_profit_percent']),\n",
    "    data_helpers.get_odds(dummy_wining_percent[2]/100, config['bookie_profit_percent']),\n",
    "]\n",
    "outcome_cols = ['outcome']\n",
    "dummy_outcome = [1, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BB(df, game_id):\n",
    "    try:\n",
    "        if game_id >= 0:\n",
    "            pair = list(df.loc[df['fixture_id'] == game_id, pair_cols].iloc[0])\n",
    "            pair_str = [str(team) for team in pair]\n",
    "            pair_text = \" \".join(pair_str)\n",
    "            pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "            print(pair_text, pair_tokens)\n",
    "            countries = list(df.loc[df['fixture_id'] == game_id, country_cols].iloc[0])\n",
    "            countries = [re.sub(r\"\\s\", \"_\", item) for item in countries]\n",
    "            countries_text = \" \".join(countries)\n",
    "            countries_tokens = tokenizer_country.encode(countries_text).ids\n",
    "            print(countries_text, countries_tokens)\n",
    "            odds = list(df.loc[df['fixture_id'] == game_id, odds_cols].iloc[0])\n",
    "            print(odds)\n",
    "        elif game_id == -1:\n",
    "            pair_text = dummy_pair\n",
    "            pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "            print(pair_text, pair_tokens)\n",
    "            countries_text = dummy_countries\n",
    "            countries_tokens = tokenizer_country.encode(countries_text).ids\n",
    "            print(countries_text, countries_tokens)      \n",
    "            odds = dummy_odds\n",
    "            print(odds)\n",
    "\n",
    "        BB = pair_tokens + countries_tokens + odds\n",
    "        BB = np.array(BB)\n",
    "        print(BB)\n",
    "        return BB\n",
    "    except:\n",
    "        raise Exception(\"Failed to get_BB. game_id: {}\".format(game_id))\n",
    "\n",
    "def get_AB(df, game_id):\n",
    "    try:\n",
    "        if game_id >= 0:\n",
    "            outcome = list(df.loc[df['fixture_id'] == game_id, outcome_cols].iloc[0])\n",
    "            outcome = (1, 0, 0) if outcome == 0 else (0, 1, 0) if outcome == 1 else (0, 0, 1)\n",
    "        elif game_id == -1:   # Hyperthetic game\n",
    "            print(\"id=-1\")\n",
    "            outcome = dummy_outcome     # [HOME] team always wins.\n",
    "            print(outcome)\n",
    "        \n",
    "        AB = outcome\n",
    "        AB = np.array(AB, dtype=np.float32)\n",
    "        print(AB)\n",
    "        return AB\n",
    "    except:\n",
    "        raise Exception(\"Failed to get_AB. game_id: {}\".format(game_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():    \n",
    "    for baseId, ids in fixture_id_to_ids.items():\n",
    "        print(baseId, ids)\n",
    "        sequence = []\n",
    "        if len(ids) > 0:\n",
    "            for id in ids:\n",
    "                BB = get_BB(df, id)\n",
    "                AB = get_AB(df, id)\n",
    "                if BB.shape[0] + AB.shape[0] > 11:\n",
    "                    print(\"shape\", baseId, ids)\n",
    "                sequence.append(np.concatenate((BB, AB)))\n",
    "                print(\"1\", sequence[-1].shape)\n",
    "        else:\n",
    "            BB = get_BB(df, -1)\n",
    "            AB = get_AB(df, -1)\n",
    "            sequence.append(np.concatenate((BB, AB)))\n",
    "            print(\"1\", sequence[-1].shape)\n",
    "\n",
    "        sequence = np.stack(sequence, axis=0)\n",
    "        print(\"shape\", sequence.shape)\n",
    "        BB = get_BB(df, int(baseId))\n",
    "        AB = get_AB(df, int(baseId))\n",
    "        if BB.shape[0] + AB.shape[0] > 11:\n",
    "            print(\"shape\", baseId, ids)\n",
    "        baseOutput = np.concatenate((BB, AB))\n",
    "\n",
    "        totalOutput = (baseOutput, sequence)\n",
    "        print(\"total: \", totalOutput)\n",
    "\n",
    "        yield totalOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "    generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=(tf.TensorShape((11,)), tf.TensorShape((None, 11))),\n",
    "    args=()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ds:\n",
    "    print(item)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(ds, \"./data/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = tf.data.experimental.load(\"./data/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ds2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
