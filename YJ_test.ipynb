{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "from config import config\n",
    "import YJ_assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "\n",
    "OPERATION = 'PRETRAIN'  # 'PRETRAIN'  'TRAIN_C'   'FINETUNE'   'TEST', 'BACKTEST'\n",
    "\n",
    "MODEL_ID = 'YJ.01' # 'YA.11'\n",
    "HISTORY_LEN = 400\n",
    "SEQ_YEAR_SPAN = 15\n",
    "NOTE = 'R_250413'\n",
    "\n",
    "DATA_THEME = str(HISTORY_LEN).zfill(3) + '-' + str(SEQ_YEAR_SPAN).zfill(2) + '-' + NOTE\n",
    "COPY_IDS_DS_DATA_FROM = None  # \"_DATA_400-15-Test\"  None\n",
    "\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 0\n",
    "TRAIN_PERCENT= 96\n",
    "VALID_PERCENT = 4\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT\n",
    "\n",
    "BATCH_SIZE = 25 # 25 for headers 6.\n",
    "\n",
    "TEAM_EMBS = 50  #\n",
    "DECODE_BASE_DATE = False\n",
    "EMBED_AB_COLS = False    # True: Pls choose a small LR so that we have plenty of train epochs and the embedded values have enough chance to seek their proper places.\n",
    "ODDS_IN_ENCODER = False   #--------------\n",
    "ODDS_IN_DECODER = True\n",
    "ADDITIONAL_D_MODEL = 0   #------------ Try increase it when underfitting.\n",
    "\n",
    "TRANSFORMER_LAYERS = 30        #----------30\n",
    "TRANSFORMER_HEADS = 6\n",
    "BALANCE_POS_CODE = True        # True: Weakens positional code compared to embedded values.\n",
    "DROPOUT = 0.0  # 0.1\n",
    "ADAPTORS_LAYERS = 0 #------------ 10\n",
    "ADAPTORS_WIDTH_FACTOR = 30  # 30\n",
    "\n",
    "# This is an exponential curve that hits STARTING_LEARNING_RATE at step zero and EXAMPLE_LEARNING_RATE at step EXAMPLE_LEARNING_STEP.\n",
    "# lr(step) = STARTING_LEARNING_RATE * pow( pow(EXAMPLE_LEARNING_RATE/STARTING_LEARNING_RATE, 1/EXAMPLE_LEARNING_STEP), step )\n",
    "STARTING_LEARNING_RATE = 5e-7\n",
    "EXAMPLE_LEARNING_RATE = STARTING_LEARNING_RATE * 0.5    #------------ 0.5 worked.\n",
    "EXAMPLE_LEARNING_STEP = 100\n",
    "\n",
    "MODEL_ACTIVATION = 'softmax'    # 'softmax', 'sigmoid', 'relu', 'open'\n",
    "LOSS_TYPE = 'profit'           # 'profit', 'entropy'\n",
    "VECTOR_BETTING = False\n",
    "STAKE_TECHNIQUE = 'independent'         # 'independent', 'majority', 'riskest'\n",
    "\n",
    "MODEL_TYPE_CHECK = False\n",
    "GET_VAL_LOSS_0 = False\n",
    "IGNORE_HISTORY = False\n",
    "SIMPLIFY_ADAPTOR = True\n",
    "NUMBER_QUERIES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#-------------------- England ---------------------\\nTime range of data: 2004/2005 - 2024/2025\\nLeagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\\n!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\\nBookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\\nWilliam Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\\nWilliam Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\\nWilliam Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\\nWilliam Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\\nWilliam Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\\nBWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTRY = 'England'\n",
    "NUMBER_BOOKIES = 5  # Take William Hills, Bet&Win, and Bet365. Other bookies' odds list change over years and leagues.\n",
    "PREFERED_ORDER = ['B365', 'WH']   # Implies the order only.\n",
    "BOOKIE_TO_EXCLUDE = []    # 'BWIN' odds don't show up since mid Febrary 2025. This may reduce the effective NUMBER_BOOKIES.\n",
    "DIVIISONS = ['E0', 'E1', 'E2', 'E3']    # 'EC', the Conference league, is excluded as some odds makers are not archived for the league since 2013.\n",
    "REPEATING_BOOKIE = None    #-----------------------------------\n",
    "\n",
    "\"\"\"\n",
    "#-------------------- England ---------------------\n",
    "Time range of data: 2004/2005 - 2024/2025\n",
    "Leagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\n",
    "!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\n",
    "Bookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\n",
    "William Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\n",
    "William Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\n",
    "William Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\n",
    "William Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\n",
    "William Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\n",
    "BWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\n",
    "\"\"\"\n",
    "\n",
    "# COUNTRY = 'Scotland'\n",
    "# NUMBER_BOOKIES = 3  # Take ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_COLS = []\n",
    "for b in range(NUMBER_BOOKIES):\n",
    "    ODDS_COLS += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ODDS_COLS\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "Required_Non_Odds_cols = Div_cols + Date_cols + Team_cols + Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "\n",
    "# Make sure Odds_cols comes first !!!\n",
    "_Cols_to_Always_Normalize = Odds_cols\n",
    "\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n",
    "\n",
    "bbab_odds_start = BBAB_cols.index(Odds_cols[0])\n",
    "bb_odds_start = BB_cols.index(Odds_cols[0])\n",
    "label_odds_start = _Label_cols.index(Odds_cols[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_folder = os.path.join('.', 'history')\n",
    "country_folder = os.path.join('.', \"data\", \"football-data-co-uk\", COUNTRY)\n",
    "gameHistory_folder_path = os.path.join(country_folder, 'Game_History')\n",
    "if not os.path.exists(gameHistory_folder_path): os.makedirs(gameHistory_folder_path, exist_ok=False)\n",
    "countryTheme_folder_path = os.path.join(country_folder, '_DATA_' + DATA_THEME)\n",
    "if not os.path.exists(countryTheme_folder_path): os.makedirs(countryTheme_folder_path, exist_ok=False)\n",
    "map_folder_path = os.path.join(countryTheme_folder_path, '_id_map')     # do not change to '_map' for backward compatibility\n",
    "if not os.path.exists(map_folder_path): os.makedirs(map_folder_path, exist_ok=False)\n",
    "dataset_foler_path = os.path.join(countryTheme_folder_path, '_dataaset')    # keep '_dataaset' as it for backward compatibility\n",
    "if not os.path.exists(dataset_foler_path): os.makedirs(dataset_foler_path, exist_ok=False)\n",
    "checkpoint_folder_path = os.path.join(country_folder, '_checkpoints')\n",
    "if not os.path.exists(checkpoint_folder_path): os.makedirs(checkpoint_folder_path, exist_ok=False)\n",
    "tokenizer_folder_path =  os.path.join(countryTheme_folder_path, '_tokenizer')\n",
    "if not os.path.exists(tokenizer_folder_path): os.makedirs(tokenizer_folder_path, exist_ok=False)\n",
    "map_filename_base = DATA_THEME\n",
    "dataset_path = os.path.join(dataset_foler_path, map_filename_base + '-foundation-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files found to rename:  0\n",
      "E1-2004-2005.xlsx E1-2005-2006.xlsx E1-2006-2007.xlsx E1-2007-2008.xlsx E1-2008-2009.xlsx E1-2009-2010.xlsx E1-2010-2011.xlsx E1-2011-2012.xlsx E1-2012-2013.xlsx E1-2013-2014.xlsx E1-2014-2015.xlsx E1-2015-2016.xlsx E1-2016-2017.xlsx E1-2017-2018.xlsx E1-2018-2019.xlsx E1-2019-2020.xlsx E1-2020-2021.xlsx E1-2021-2022.xlsx E1-2022-2023.xlsx E1-2023-2024.xlsx E1-2024-2025.xlsx E2-2004-2005.xlsx E2-2005-2006.xlsx E2-2006-2007.xlsx E2-2007-2008.xlsx E2-2008-2009.xlsx E2-2009-2010.xlsx E2-2010-2011.xlsx E2-2011-2012.xlsx E2-2012-2013.xlsx E2-2013-2014.xlsx E2-2014-2015.xlsx E2-2015-2016.xlsx E2-2016-2017.xlsx E2-2017-2018.xlsx E2-2018-2019.xlsx E2-2019-2020.xlsx E2-2020-2021.xlsx E2-2021-2022.xlsx E2-2022-2023.xlsx E2-2023-2024.xlsx E2-2024-2025.xlsx E3-2004-2005.xlsx E3-2005-2006.xlsx E3-2006-2007.xlsx E3-2007-2008.xlsx E3-2008-2009.xlsx E3-2009-2010.xlsx E3-2010-2011.xlsx E3-2011-2012.xlsx E3-2012-2013.xlsx E3-2013-2014.xlsx E3-2014-2015.xlsx E3-2015-2016.xlsx E3-2016-2017.xlsx E3-2017-2018.xlsx E3-2018-2019.xlsx E3-2019-2020.xlsx E3-2020-2021.xlsx E3-2021-2022.xlsx E3-2022-2023.xlsx E3-2023-2024.xlsx E3-2024-2025.xlsx E0-2004-2005.xlsx E0-2005-2006.xlsx E0-2006-2007.xlsx E0-2007-2008.xlsx E0-2008-2009.xlsx E0-2009-2010.xlsx E0-2010-2011.xlsx E0-2011-2012.xlsx E0-2012-2013.xlsx E0-2013-2014.xlsx E0-2014-2015.xlsx E0-2015-2016.xlsx E0-2016-2017.xlsx E0-2017-2018.xlsx E0-2018-2019.xlsx E0-2019-2020.xlsx E0-2020-2021.xlsx E0-2021-2022.xlsx E0-2022-2023.xlsx E0-2023-2024.xlsx E0-2024-2025.xlsx \n",
      "84  files found\n",
      "creating an empty df_new...\n",
      "df_train:  (42229, 38)\n",
      "df_new:  (0, 38)\n"
     ]
    }
   ],
   "source": [
    "#========================= CREATE_DADAFRAMES_v2 =============================\n",
    "\n",
    "skip = True\n",
    "YJ_assistant.assign_seasonal_filenames(gameHistory_folder_path)\n",
    "df_train, df_new = YJ_assistant.CREATE_DADAFRAMES_v2(gameHistory_folder_path, countryTheme_folder_path, Required_Non_Odds_cols, NUMBER_BOOKIES, oddsGroupsToExclude = BOOKIE_TO_EXCLUDE, preferedOrder = PREFERED_ORDER, train_mode = TRAIN_MODE, skip=skip)\n",
    "print(\"df_train: \", df_train.shape)\n",
    "print(\"df_new: \", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_list = list(df_train['Div']); home_list = list(df_train['HomeTeam']); away_list = list(df_train['AwayTeam'])\n",
    "unique_divs = set(div_list)\n",
    "\n",
    "teams_by_div = { div : set([team for team in home_list if div_list[home_list.index(team)]==div]).union(set([team for team in away_list if div_list[away_list.index(team)]==div])) for div in unique_divs }\n",
    "for div1 in unique_divs:\n",
    "    for div2 in unique_divs:\n",
    "        if div1 != div2:\n",
    "            assert teams_by_div[div1].intersection(teams_by_div[div2]) == set()\n",
    "teams_by_div = { div: list(teams) for (div, teams) in teams_by_div.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "\n",
    "def CREATE_MAP_v2(folder, idMap_filename, targetLength, df_sequence, df_base, year_span, testcount=-1, to_save=True):\n",
    "\n",
    "        def find_Electric_Flow_On_Connected_Graph(graph, source, target, inpuCurrent):\n",
    "                '''\n",
    "                Find the flow of electric current on each edge of 'graph' when total flow of 1.0 flows from 'source' to 'target' nodes,\n",
    "                with conductance of edges stored in edge['conductance'].\n",
    "                1. If 'source' and 'target' nodes are disconnected with each other, WEIRD amounts on flows on edges.\n",
    "                2. If 'source' and 'target' nodes have zero conductance between them, it produces WEIRD amount of flows that violate Kirchhoff's law.\n",
    "                '''\n",
    "                edges = [e for e in graph.edges]        # (u, v) either u < v or u < v.\n",
    "                nodes = [v for v in graph.nodes]\n",
    "                edges_signs = []\n",
    "                for v in nodes:\n",
    "                        # We want the orientation of an edge (u, v) to be from min(u, v) to max(u, v)\n",
    "                        edges_v = [(v, u) for u in nodes if (v, u) in graph.edges]      # all edges that have v as its node, in the form of (v,u)\n",
    "                        edges_v_plus = [(v, u) for (v, u) in edges_v if v > u]          # (v, u < v)\n",
    "                        edges_v_minus = [(v, u) for (v, u) in edges_v if v < u]         # (v, u >= v)\n",
    "                        edges_v_plus = [(edges.index((u, v)) if edges.count((u, v)) > 0 else edges.index((v, u))) for (u, v) in edges_v_plus]\n",
    "                        edges_v_minus = [(edges.index((u, v)) if edges.count((u, v)) > 0 else edges.index((v, u))) for (u, v) in edges_v_minus]\n",
    "                        edges_signs.append((edges_v_plus, edges_v_minus))\n",
    "                matrix_B = [ [ (1 if e in edges_signs[v][1] else -1 if e in edges_signs[v][0] else 0) for e in range(len(edges)) ] for v in range(len(nodes))]\n",
    "                matrix_B = np.array(matrix_B, dtype=np.float32)\n",
    "                vector_C = np.array( [graph.edges[e]['conductance'] for e in edges], dtype=np.float32)\n",
    "                matrix_C = np.diag(vector_C)\n",
    "                matrix_L = np.dot(np.dot(matrix_B, matrix_C), matrix_B.T)\n",
    "                inverse_L = np.linalg.pinv(matrix_L, hermitian=True)    # Moore-Penrose pseudo-inverse\n",
    "                source_node = nodes.index(source)\n",
    "                target_node = nodes.index(target)\n",
    "                X_vector = np.zeros((len(nodes),), dtype=np.float32)\n",
    "                X_vector[source_node] = inpuCurrent\n",
    "                X_vector[target_node] = - inpuCurrent\n",
    "                flows = np.matmul(matrix_C, matrix_B.T)\n",
    "                flows = np.matmul(flows, inverse_L)\n",
    "                flows = np.matmul(flows, X_vector)\n",
    "\n",
    "                return flows, nodes, edges   # nodes and edges, just in case list(graph.nodes) might have different order each time.\n",
    "\n",
    "        def find_nTotalGames(gameGraph):\n",
    "                nTotalGames = 0\n",
    "                for e in gameGraph.edges:   nTotalGames += len(gameGraph.edges[e]['games'])\n",
    "                return nTotalGames\n",
    "\n",
    "        def reachable(graph, u, v):\n",
    "                _reachable = True\n",
    "                try:    nx.shortest_path_length(graph, u, v)\n",
    "                except: _reachable = False\n",
    "                return _reachable\n",
    "\n",
    "        def isConnected(gameGraph):\n",
    "                connected = True\n",
    "                for u in gameGraph.nodes:\n",
    "                        for v in gameGraph.nodes:\n",
    "                                if not reachable(gameGraph, u, v):\n",
    "                                        connected = False\n",
    "                                        break\n",
    "                return connected\n",
    "\n",
    "        def create_conducting_game_graph_uk(game_list, baseDate, conductance365):\n",
    "                # game_list is sorted in (date, div).\n",
    "                graph = nx.Graph()\n",
    "                alpha = pow(conductance365, -1/365)\n",
    "                def conductance(_date):\n",
    "                        daysAgo = (baseDate - _date).days       # > 0\n",
    "                        return pow(alpha, -daysAgo)             # pow(conductance365, daysAgo/365) <= 1, as conductance365 < 1\n",
    "\n",
    "                for id, div, home, away, dt in game_list:\n",
    "                        if (home, away) not in graph.edges:\n",
    "                                graph.add_edge(home, away)\n",
    "                                graph.edges[home, away]['games'] = []\n",
    "                        graph.edges[home, away]['games'].append((id, dt, conductance(dt)))\n",
    "\n",
    "                for edge in graph.edges:\n",
    "                        games = graph.edges[edge]['games']\n",
    "                        edge_con = 0.0\n",
    "                        for (_, _, con) in games: edge_con += con\n",
    "                        graph.edges[edge]['conductance'] = edge_con\n",
    "\n",
    "                return graph\n",
    "\n",
    "\n",
    "        def find_games(gameGraph):\n",
    "                games = []\n",
    "                for e in gameGraph.edges:   games += [id for (id, date, con) in gameGraph.edges[e]['games']]\n",
    "                return games\n",
    "\n",
    "        def try_remove_lowest_flow_games(gameGraph, eFlows, edges, targetLength):\n",
    "                \"\"\"\n",
    "                1. Eithe isConnected(gameGraph) or not. This call worsens connectivity by removing some edges.\n",
    "                2. Very small e-current incidently created by numerical errors unexpectedly affect this function.\n",
    "                \"\"\" \n",
    "                emptyPairs = []\n",
    "                currents = []\n",
    "                for (teamA, teamB) in gameGraph.edges:  # May not: teamA < teamB. PairExpression_1 is created here.\n",
    "                        # get the current on the edge (teamA, teamB)\n",
    "                        eId = (edges.index((teamA, teamB)) if edges.count((teamA, teamB)) > 0 else edges.index((teamB, teamA))) # Exists.\n",
    "                        flow = abs(eFlows[eId]) # May be very small. NP.\n",
    "                        \n",
    "                        if len(gameGraph.edges[teamA, teamB]['games']) <= 0:\n",
    "                                emptyPairs.append((teamA, teamB))       # PairExpression_1\n",
    "                        else:   # Enlist all games no matter how small e-current flows on them.\n",
    "                                edge_conductance = gameGraph.edges[teamA, teamB]['conductance'] # Asserted positive, no need epsilon.\n",
    "                                currentPerUnitCon = flow / edge_conductance\n",
    "                                pair_games = gameGraph.edges[teamA, teamB]['games']     # Creates a set of pair expressions. Should be PairExpression_1, if deterministic.\n",
    "                                pair_current = [(currentPerUnitCon * cond, id, date, cond, teamA, teamB) for (id, date, cond) in pair_games] # PairExpression_1\n",
    "                                currents += pair_current\n",
    "\n",
    "                # Note: Pair representations (A, B) in currents came from [for (A, B) in gameGraph.edges]\n",
    "                for (teamA, teamB) in emptyPairs:   gameGraph.remove_edge(teamA, teamB)\n",
    "                assert find_nTotalGames(gameGraph) == len(currents)\n",
    "\n",
    "                # sort currents in (curr / descending, date / descending )\n",
    "                currents = [(date, curr, id, cond, teamA, teamB) for (curr, id, date, cond, teamA, teamB) in currents]\n",
    "                currents.sort(reverse=True)     # later dates come first\n",
    "                currents = [(curr, id, date, cond, teamA, teamB) for (date, curr, id, cond, teamA, teamB) in currents]\n",
    "                currents.sort(reverse=True)    # larger current comes first. PairExpression_1\n",
    "\n",
    "                if len(currents) <= targetLength:   pass        # This doens't happen becasue we enlisted ALL games above.\n",
    "                else:\n",
    "                        # Either zero-current edges survive or positive-current edges are removed by this cut, both leading to disconnected graph. \n",
    "                        currents = currents[ : targetLength ]   # note currents are sorted in (curr, date)\n",
    "                        pairsChanged = []\n",
    "                        #-----------------------------------------------------------------------------------------------------\n",
    "                        #   Below, 'currents' is reflected to gameGraph. No more pairs/games are removed, except that.\n",
    "                        #-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "                        #======== Find <which games on which pair> are in 'currents'\n",
    "                        pairsInCurrents = list(set([(teamA, teamB) for (_,_,_,_, teamA, teamB) in currents]))   # PairExpression_1\n",
    "                        gamesByPairInCurrents = [ ( (teamA, teamB), [(id, date, con) for (_, id, date, con, _teamA, _teamB) in currents # PairExpression_1\n",
    "                                if _teamA == teamA and _teamB == teamB ] )      # currents and pairs_from_current share the same expressions of pair.\n",
    "                                for (teamA, teamB) in pairsInCurrents]       # May not: teamA < teamB\n",
    "\n",
    "                        #========= Remove existing pairs that are not in pairsInCurrents, that has no game at all in currents.\n",
    "                        allPairs = [(teamA, teamB) for (teamA, teamB) in gameGraph.edges]   # PairExpression_2 is created here.\n",
    "                        pairsToRemove = [(teamA, teamB) for (teamA, teamB) in allPairs if ((teamA, teamB) not in pairsInCurrents and (teamB, teamA) not in pairsInCurrents)]\n",
    "                        for (teamA, teamB) in pairsToRemove:  gameGraph.remove_edge(teamA, teamB)\n",
    "\n",
    "                        #========= Replace existing games of pairsInCurrents with games found in 'currents' if appropriate.\n",
    "                        pairsChanged = []\n",
    "                        for ((teamA, teamB), games) in gamesByPairInCurrents: # PairExpression_1\n",
    "                                if len(gameGraph.edges[teamA, teamB]['games']) != len(games):   # if some games were excluded.\n",
    "                                        gameGraph.edges[teamA, teamB]['games'] = games  # Replace.\n",
    "                                        pairsChanged.append((teamA, teamB))\n",
    "\n",
    "                        #========= Update nTotalGames\n",
    "                        nTotalGames = find_nTotalGames(gameGraph)\n",
    "                        assert nTotalGames == targetLength      # because we enlisted all games.\n",
    "\n",
    "                return gameGraph, nTotalGames, pairsChanged, currents\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        div_list = list(df_sequence['Div']); home_list = list(df_sequence['HomeTeam']); away_list = list(df_sequence['AwayTeam'])\n",
    "        unique_divs = set(div_list)\n",
    "\n",
    "        teams_by_div = { div : set([team for team in home_list if div_list[home_list.index(team)]==div]).union(set([team for team in away_list if div_list[away_list.index(team)]==div])) for div in unique_divs }\n",
    "        for div1 in unique_divs:\n",
    "                for div2 in unique_divs:\n",
    "                        if div1 != div2:\n",
    "                                assert teams_by_div[div1].intersection(teams_by_div[div2]) == set()\n",
    "        teams_by_div = { div: list(teams) for (div, teams) in teams_by_div.items()}\n",
    "        #--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        def get_historical_games_intra_div(base_id, base_date, base_div, home, away, div_sub_list, history_len, inputCurrent, conductance365):\n",
    "                \"\"\"\n",
    "                Goal: Choose as many as, most relevant, games from div_sub_list\n",
    "                \"\"\"\n",
    "                games = []; seq_type = 0\n",
    "                if len(div_sub_list) <= history_len:\n",
    "                        games = [id for (id, _, _, _, _) in div_sub_list]\n",
    "                        seq_type = 10\n",
    "                else:\n",
    "                        dgg = create_conducting_game_graph_uk(div_sub_list, base_date, conductance365=conductance365)   # MUCH faster than transform_to_conducting_graph\n",
    "\n",
    "                        if reachable(dgg, home, away):\n",
    "                                flows, nodes, edges = find_Electric_Flow_On_Connected_Graph(dgg, home, away, inputCurrent)\n",
    "                                flows_copy = copy.deepcopy(flows)\n",
    "                                flows_copy = [abs(f) for f in flows_copy]\n",
    "                                flows_copy.sort(reverse=True)\n",
    "\n",
    "                                if  flows_copy[0] > inputCurrent/100:   # e-current tach seems successful.\n",
    "                                        # Now, either isConnected(dgg) or not. This call worsens it by removing some edges.\n",
    "                                        dgg, _, _, _ = try_remove_lowest_flow_games(dgg, flows, edges, history_len)\n",
    "                                        games = find_games(dgg)\n",
    "                                        assert len(games) == history_len\n",
    "                                        seq_type = 20\n",
    "                                else:   # e-current tech failed.\n",
    "                                        games = [id for (id, _, _, _, _) in div_sub_list[- history_len : ]]  # collect latest ids\n",
    "                                        seq_type = 30\n",
    "\n",
    "                        else: # Very rare. Few games are new edge in the conductance graph after collecting at lease HISTORY_LEN past games in the graph. They might be inter-league games.\n",
    "                                # find_Electric_Flow_On_Connected_Graph(.) doesn't work here.\n",
    "                                games = [id for (id, _,_,_,_) in div_sub_list[- history_len : ]]   # collect latest ids\n",
    "                                seq_type = 40\n",
    "\n",
    "                return games, seq_type\n",
    "        \n",
    "             \n",
    "     \n",
    "        def get_historical_games_v2(base_id, base_date, base_div, home, away, sub_list, history_len, inputCurrent, conductance365):\n",
    "                games = []; report = None\n",
    "                if len(sub_list) <= history_len:\n",
    "                        games = [id for (id, _, _, _, _) in sub_list]      # better than dummy games.\n",
    "                        seq_type = 1\n",
    "                else:\n",
    "                        # This logic is based on the emperical proof that a team plays in only a division. Divisions, as a set of teams, have no intersection.\n",
    "\n",
    "                        div_sub_list = [(id, div, home, away, dt) for (id, div, home, away, dt) in sub_list if div == base_div]\n",
    "                        games, seq_type = get_historical_games_intra_div(base_id, base_date, base_div, home, away, div_sub_list, history_len, inputCurrent, conductance365)\n",
    "\n",
    "                        if len(games) < history_len:\n",
    "                                candi_games = [id for (id, _, _, _, _) in sub_list if id not in games]  # We know sub_list is ascendigly sorted in date.\n",
    "                                games = games + candi_games[ - (history_len - len(games)) : ]         # So this chooses latest games.\n",
    "                                seq_type += 1\n",
    "                        assert len(games) == history_len\n",
    "                        # if not isConnected(gg): report += 1000         # expensive\n",
    "\n",
    "                games.sort(reverse=True)        # latest games come first.\n",
    "                return games, seq_type\n",
    "        \n",
    "        def sort_id_to_ids(id_to_ids):\n",
    "                dates_ids = [(baseId, report, games) for (baseId, (report, games)) in id_to_ids.items()]\n",
    "                dates_ids.sort()        # increasing on baseId\n",
    "                # print('dates_ids', dates_ids)\n",
    "                id_to_ids = {int(baseId): (report, games) for (baseId, report, games) in dates_ids}\n",
    "                return id_to_ids\n",
    "        \n",
    "        def save_step_id_to_ids(path, step_id_to_ids, work_id_to_ids, old_step, to_save):\n",
    "                save = {}\n",
    "                if old_step >= 0:\n",
    "                        if len(work_id_to_ids) > 0:   # Don't save anew unless we have extra id_to_ids, because this saving sometimes saves a wrong file.'\"Electrical Flows 2.pdf\"\n",
    "                                save = step_id_to_ids | sort_id_to_ids(work_id_to_ids)\n",
    "                                if to_save: YJ_assistant.SaveJsonData(save, path)\n",
    "                        else:\n",
    "                                save = step_id_to_ids\n",
    "                return save\n",
    "\n",
    "        #=========================================================================== Main =======================================================================\n",
    "\n",
    "        id_list = list(df_sequence['id']); div_list = list(df_sequence['Div']); home_list = list(df_sequence['HomeTeam']); away_list = list(df_sequence['AwayTeam']); date_list = list(df_sequence['Date'])\n",
    "        total_list = list(zip(id_list, div_list, home_list, away_list, date_list))\n",
    "\n",
    "        id_list_s = list(df_base['id']); div_list_s = list(df_base['Div']); home_list_s = list(df_base['HomeTeam']); away_list_s = list(df_base['AwayTeam']); date_list_s = list(df_base['Date'])\n",
    "        search_list = list(zip(id_list_s, div_list_s, home_list_s, away_list_s, date_list_s))\n",
    "\n",
    "        step_size = int(1E3)        # do not change.\n",
    "        old_step = -1\n",
    "        total_id_to_ids = {}\n",
    "        step_id_to_ids = {}\n",
    "        work_id_to_ids = {}\n",
    "\n",
    "        # df_built = df_built.sort_values(['Date', 'Div'], ascending=[True, True])\n",
    "        \n",
    "        max_days_covered = 0; count = 0\n",
    "        for (base_id, base_div, home, away, base_date) in search_list:      # date: yyyy-mm-dd        , in search_list\n",
    "                if count == testcount: break\n",
    "                count += 1\n",
    "\n",
    "                step = int(base_id/step_size) * step_size   # sure step >= 0\n",
    "\n",
    "                def build_path(step):\n",
    "                        return os.path.join(folder, idMap_filename + '-step-' + str(step) + '-size-' + str(step_size) + '.json')\n",
    "\n",
    "                # Note the final step is always not saved. Save it after this loop.\n",
    "                if step != old_step:    # We are turning to a new step.\n",
    "                        save = save_step_id_to_ids(build_path(old_step), step_id_to_ids, work_id_to_ids, old_step, to_save)\n",
    "                        total_id_to_ids = total_id_to_ids | save\n",
    "                        step_id_to_ids = {}\n",
    "                        path = build_path(step)\n",
    "                        id_to_ids_read = YJ_assistant.LoadJsonData(path)\n",
    "                        if id_to_ids_read is not None:\n",
    "                                step_id_to_ids = id_to_ids_read\n",
    "                        work_id_to_ids = {}\n",
    "                        old_step = step\n",
    "                \n",
    "                if str(base_id) in step_id_to_ids.keys():  continue\n",
    "\n",
    "                #------------------------------------------------------------------------------------------------------- Goal: get games.\n",
    "                #????????????????????????????????????? Shall we limit the list to max 5 years ?????????????????????????????????????????????????\n",
    "                day_span = year_span * 365\n",
    "                sub_list = [(id, div, home, away, dt) for (id, div, home, away, dt) in total_list if id < base_id and (base_date-dt).days <= day_span]\n",
    "                # div_sub_list = [(id, div, home, away, dt) for (id, div, home, away, dt) in sub_list if div == base_div]\n",
    "\n",
    "                inputCurrent = 1000.0\n",
    "                games, report = get_historical_games_v2(base_id, base_date, base_div, home, away, sub_list, targetLength, inputCurrent, conductance365=0.9)\n",
    "\n",
    "                if len(games) > 0: days_covered = (base_date - date_list[id_list.index(games[-1])]).days\n",
    "                else: days_covered = 0\n",
    "                if max_days_covered < days_covered: max_days_covered = days_covered\n",
    "\n",
    "                print(\"base_id: {}, report: {}, days_span: {}, games[:10]: {}\" \\\n",
    "                      .format(base_id, report, days_covered, games[:10]), end='\\r')\n",
    "                #-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "                if len(games) >= 0: work_id_to_ids[base_id] = (report, games)\n",
    "\n",
    "        # Give a chance to the final step to save.\n",
    "        save = save_step_id_to_ids(build_path(old_step), step_id_to_ids, work_id_to_ids, old_step, to_save)\n",
    "        total_id_to_ids = total_id_to_ids | save\n",
    "\n",
    "        # print(len(total_id_to_ids))\n",
    "        total_id_to_ids = { id : value for (id, value) in total_id_to_ids.items() if int(id) in id_list_s }\n",
    "\n",
    "        return total_id_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_id: 1009472, report: 20, days_span: 308, games[:10]: [1009471, 1009470, 1009469, 1009468, 1009447, 1009446, 1009445, 1009444, 1009437, 1009436]]\r"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODE:\n",
    "    targetLength = HISTORY_LEN\n",
    "\n",
    "    df_sequence = df_train; df_base = df_train\n",
    "    train_map = CREATE_MAP_v2(map_folder_path, map_filename_base, targetLength, df_sequence, df_base, year_span=SEQ_YEAR_SPAN, testcount=-1)\n",
    "\n",
    "    print(\"\\nlen = \", len(train_map))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
